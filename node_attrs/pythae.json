{
 "PRed": [
  {
   "PR": {
    "__lazy_json__": "pr_json/986822955.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "Version",
    "migrator_version": 0,
    "version": "0.0.2"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "version"
   ]
  }
 ],
 "archived": false,
 "bad": false,
 "branch": "main",
 "conda-forge.yml": {
  "bot": {
   "automerge": true
  }
 },
 "feedstock_name": "pythae",
 "hash_type": "sha256",
 "linux_64_meta_yaml": {
  "about": {
   "description": "This library implements some of the most common (Variational) Autoencoder models.\nIn particular it provides the possibility to perform benchmark experiments and\ncomparisons by training the models with the same autoencoding neural network\narchitecture. The feature *make your own autoencoder* allows you to train any of\nthese models with your own data and own Encoder and Decoder neural networks.\n\nPyPI: [https://pypi.org/project/pythae](https://pypi.org/project/pythae)\n",
   "dev_url": "https://github.com/clementchadebec/benchmark_VAE",
   "doc_url": "https://pythae.readthedocs.io/en/latest/",
   "home": "https://github.com/clementchadebec/benchmark_VAE",
   "license": "Apache-2.0",
   "license_file": "LICENSE",
   "summary": "Unifying Generative Autoencoders in Python"
  },
  "build": {
   "noarch": "python",
   "number": "1",
   "script": "/usr/share/miniconda3/envs/run_env/conda-bld/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pla/bin/python -m pip install . -vv"
  },
  "extra": {
   "recipe-maintainers": [
    "sugatoray"
   ]
  },
  "package": {
   "name": "pythae",
   "version": "0.0.2"
  },
  "requirements": {
   "host": [
    "pip",
    "python >=3.6"
   ],
   "run": [
    "python >=3.6",
    "dataclasses >=0.6",
    "dill >=0.3.3",
    "imageio",
    "numpy >=1.19",
    "pydantic >=1.8.2",
    "scipy >=1.7.1",
    "scikit-learn",
    "pytorch >=1.10.1",
    "tqdm",
    "typing-extensions"
   ]
  },
  "source": {
   "sha256": "018b5dd9a232c0186fe71ebbaa7b023dfa6a1df817a004996eeb27b8b59f748b",
   "url": "https://pypi.io/packages/source/p/pythae/pythae-0.0.2.tar.gz"
  },
  "test": {
   "imports": [
    "pythae"
   ]
  }
 },
 "linux_64_requirements": {
  "build": {
   "__set__": true,
   "elements": []
  },
  "host": {
   "__set__": true,
   "elements": [
    "pip",
    "python"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "dataclasses",
    "dill",
    "imageio",
    "numpy",
    "pydantic",
    "python",
    "pytorch",
    "scikit-learn",
    "scipy",
    "tqdm",
    "typing-extensions"
   ]
  },
  "test": {
   "__set__": true,
   "elements": []
  }
 },
 "meta_yaml": {
  "about": {
   "description": "This library implements some of the most common (Variational) Autoencoder models.\nIn particular it provides the possibility to perform benchmark experiments and\ncomparisons by training the models with the same autoencoding neural network\narchitecture. The feature *make your own autoencoder* allows you to train any of\nthese models with your own data and own Encoder and Decoder neural networks.\n\nPyPI: [https://pypi.org/project/pythae](https://pypi.org/project/pythae)\n",
   "dev_url": "https://github.com/clementchadebec/benchmark_VAE",
   "doc_url": "https://pythae.readthedocs.io/en/latest/",
   "home": "https://github.com/clementchadebec/benchmark_VAE",
   "license": "Apache-2.0",
   "license_file": "LICENSE",
   "summary": "Unifying Generative Autoencoders in Python"
  },
  "build": {
   "noarch": "python",
   "number": "1",
   "script": "/usr/share/miniconda3/envs/run_env/conda-bld/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pla/bin/python -m pip install . -vv"
  },
  "extra": {
   "recipe-maintainers": [
    "sugatoray"
   ]
  },
  "package": {
   "name": "pythae",
   "version": "0.0.2"
  },
  "requirements": {
   "host": [
    "pip",
    "python >=3.6"
   ],
   "run": [
    "python >=3.6",
    "dataclasses >=0.6",
    "dill >=0.3.3",
    "imageio",
    "numpy >=1.19",
    "pydantic >=1.8.2",
    "scipy >=1.7.1",
    "scikit-learn",
    "pytorch >=1.10.1",
    "tqdm",
    "typing-extensions"
   ]
  },
  "source": {
   "sha256": "018b5dd9a232c0186fe71ebbaa7b023dfa6a1df817a004996eeb27b8b59f748b",
   "url": "https://pypi.io/packages/source/p/pythae/pythae-0.0.2.tar.gz"
  },
  "test": {
   "imports": [
    "pythae"
   ]
  }
 },
 "name": "pythae",
 "new_version": "0.0.2",
 "new_version_attempts": {
  "0.0.2": 1
 },
 "new_version_errors": {},
 "outputs_names": {
  "__set__": true,
  "elements": [
   "pythae"
  ]
 },
 "pinning_version": "2022.07.03.20.57.18",
 "raw_meta_yaml": "{% set reqfile = \"requirements.txt\" %}\n{% set name = \"pythae\" %}\n{% set version = \"0.0.2\" %}\n\n\npackage:\n  name: {{ name|lower }}\n  version: {{ version }}\n\nsource:\n  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/pythae-{{ version }}.tar.gz\n  sha256: 018b5dd9a232c0186fe71ebbaa7b023dfa6a1df817a004996eeb27b8b59f748b\n\nbuild:\n  number: 1\n  noarch: python\n  script: {{ PYTHON }} -m pip install . -vv\n\nrequirements:\n  host:\n    - pip\n    - python >=3.6\n  run:\n    - python >=3.6\n    - dataclasses >=0.6\n    - dill >=0.3.3\n    - imageio\n    - numpy >=1.19\n    - pydantic >=1.8.2\n    - scipy >=1.7.1\n    - scikit-learn\n    - pytorch >=1.10.1\n    - tqdm\n    - typing-extensions\n\ntest:\n  imports:\n    - pythae\n  # commands:\n  #   - pip check\n  # requires:\n  #   - pip\n\nabout:\n  home: https://github.com/clementchadebec/benchmark_VAE\n  summary: Unifying Generative Autoencoders in Python\n  license: Apache-2.0\n  license_file: LICENSE\n  description: |\n    This library implements some of the most common (Variational) Autoencoder models. \n    In particular it provides the possibility to perform benchmark experiments and \n    comparisons by training the models with the same autoencoding neural network \n    architecture. The feature *make your own autoencoder* allows you to train any of \n    these models with your own data and own Encoder and Decoder neural networks.\n\n    PyPI: [https://pypi.org/project/pythae](https://pypi.org/project/pythae)\n\n  doc_url: https://pythae.readthedocs.io/en/latest/\n  dev_url: https://github.com/clementchadebec/benchmark_VAE\n\nextra:\n  recipe-maintainers:\n    - sugatoray\n",
 "req": {
  "__set__": true,
  "elements": [
   "dataclasses",
   "dill",
   "imageio",
   "numpy",
   "pip",
   "pydantic",
   "python",
   "pytorch",
   "scikit-learn",
   "scipy",
   "tqdm",
   "typing-extensions"
  ]
 },
 "requirements": {
  "build": {
   "__set__": true,
   "elements": []
  },
  "host": {
   "__set__": true,
   "elements": [
    "pip",
    "python"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "dataclasses",
    "dill",
    "imageio",
    "numpy",
    "pydantic",
    "python",
    "pytorch",
    "scikit-learn",
    "scipy",
    "tqdm",
    "typing-extensions"
   ]
  },
  "test": {
   "__set__": true,
   "elements": []
  }
 },
 "smithy_version": "3.21.0",
 "strong_exports": false,
 "total_requirements": {
  "build": {
   "__set__": true,
   "elements": []
  },
  "host": {
   "__set__": true,
   "elements": [
    "pip",
    "python >=3.6"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "dataclasses >=0.6",
    "dill >=0.3.3",
    "imageio",
    "numpy >=1.19",
    "pydantic >=1.8.2",
    "python >=3.6",
    "pytorch >=1.10.1",
    "scikit-learn",
    "scipy >=1.7.1",
    "tqdm",
    "typing-extensions"
   ]
  },
  "test": {
   "__set__": true,
   "elements": []
  }
 },
 "url": "https://pypi.io/packages/source/p/pythae/pythae-0.0.2.tar.gz",
 "version": "0.0.2"
}