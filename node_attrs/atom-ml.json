{
 "PRed": [
  {
   "PR": {
    "__lazy_json__": "pr_json/546202079.json"
   },
   "data": {
    "bot_rerun": 1609242274.764256,
    "migrator_name": "Version",
    "migrator_version": 0,
    "version": "4.2.0"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "version"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/582878960.json"
   },
   "data": {
    "bot_rerun": 1617028650.7733576,
    "migrator_name": "Version",
    "migrator_version": 0,
    "version": "4.3.0"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "version"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/731990005.json"
   },
   "data": {
    "bot_rerun": 1631433358.8918684,
    "migrator_name": "Version",
    "migrator_version": 0,
    "version": "4.7.3"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "version"
   ]
  }
 ],
 "archived": false,
 "bad": false,
 "branch": "main",
 "conda-forge.yml": {},
 "feedstock_name": "atom-ml",
 "hash_type": "sha256",
 "linux_64_meta_yaml": {
  "about": {
   "description": "During the exploration phase of a machine learning project, a data\nscientist tries to find the optimal pipeline for his specific use case.\nThis usually involves applying standard data cleaning steps, creating\nor selecting useful features, trying out different models, etc. Testing\nmultiple pipelines requires many lines of code, and writing it all in\nthe same notebook often makes it long and cluttered. On the other hand,\nusing multiple notebooks makes it harder to compare the results and to\nkeep an overview. On top of that, refactoring the code for every test\ncan be time-consuming. How many times have you conducted the same action\nto pre-process a raw dataset? How many times have you copy-and-pasted\ncode from an old repository to re-use it in a new use case?\n\nATOM is here to help solve these common issues. The package acts as\na wrapper of the whole machine learning pipeline, helping the data\nscientist to rapidly find a good model for his problem. Avoid\nendless imports and documentation lookups. Avoid rewriting the same\ncode over and over again. With just a few lines of code, it's now\npossible to perform basic data cleaning steps, select relevant\nfeatures and compare the performance of multiple models on a given\ndataset, providing quick insights on which pipeline performs best\nfor the task at hand.\n",
   "dev_url": "http://github.com/tvdboom/ATOM/tree/development",
   "doc_url": "https://tvdboom.github.io/ATOM/",
   "home": "http://github.com/tvdboom/ATOM",
   "license": "MIT",
   "license_file": "LICENSE",
   "summary": "A Python package for fast exploration of machine learning pipelines"
  },
  "build": {
   "noarch": "python",
   "number": "0",
   "script": "/usr/share/miniconda3/envs/run_env/conda-bld/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pla/bin/python -m pip install . --no-deps -vv"
  },
  "extra": {
   "recipe-maintainers": [
    "tvdboom"
   ]
  },
  "package": {
   "name": "atom-ml",
   "version": "4.8.0"
  },
  "requirements": {
   "host": [
    "python >=3.6",
    "pip"
   ],
   "run": [
    "python >=3.6",
    "numpy",
    "scipy",
    "pandas",
    "pandas-profiling",
    "mlflow",
    "dill",
    "tqdm",
    "joblib",
    "typeguard",
    "tabulate",
    "scikit-learn ~= 0.24.0",
    "scikit-optimize",
    "nltk",
    "tpot",
    "category_encoders",
    "imbalanced-learn",
    "featuretools",
    "gplearn",
    "matplotlib-base",
    "seaborn",
    "shap",
    "wordcloud"
   ]
  },
  "source": {
   "sha256": "0cd760cf0f48dbde17500f20bdb413fceef0e4247640b13f88627a87fbb74ee9",
   "url": "https://pypi.io/packages/source/a/atom-ml/atom-ml-4.8.0.tar.gz"
  },
  "test": {
   "commands": [
    "mkdir tests/files",
    "coverage run -m pytest"
   ],
   "imports": [
    "atom"
   ],
   "requires": [
    "pip",
    "pytest",
    "coverage",
    "python",
    "tensorflow",
    "numpy",
    "scipy",
    "pandas",
    "pandas-profiling",
    "mlflow",
    "dill",
    "tqdm",
    "joblib",
    "typeguard",
    "tabulate",
    "scikit-learn ~= 0.24.0",
    "scikit-optimize",
    "nltk",
    "tpot",
    "category_encoders",
    "imbalanced-learn",
    "featuretools",
    "gplearn",
    "matplotlib-base",
    "seaborn",
    "shap",
    "wordcloud",
    "py-xgboost",
    "lightgbm",
    "catboost"
   ],
   "source_files": [
    "tests"
   ]
  }
 },
 "linux_64_requirements": {
  "build": {
   "__set__": true,
   "elements": []
  },
  "host": {
   "__set__": true,
   "elements": [
    "pip",
    "python"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "category_encoders",
    "dill",
    "featuretools",
    "gplearn",
    "imbalanced-learn",
    "joblib",
    "matplotlib-base",
    "mlflow",
    "nltk",
    "numpy",
    "pandas",
    "pandas-profiling",
    "python",
    "scikit-learn",
    "scikit-optimize",
    "scipy",
    "seaborn",
    "shap",
    "tabulate",
    "tpot",
    "tqdm",
    "typeguard",
    "wordcloud"
   ]
  },
  "test": {
   "__set__": true,
   "elements": [
    "catboost",
    "category_encoders",
    "coverage",
    "dill",
    "featuretools",
    "gplearn",
    "imbalanced-learn",
    "joblib",
    "lightgbm",
    "matplotlib-base",
    "mlflow",
    "nltk",
    "numpy",
    "pandas",
    "pandas-profiling",
    "pip",
    "py-xgboost",
    "pytest",
    "python",
    "scikit-learn",
    "scikit-optimize",
    "scipy",
    "seaborn",
    "shap",
    "tabulate",
    "tensorflow",
    "tpot",
    "tqdm",
    "typeguard",
    "wordcloud"
   ]
  }
 },
 "meta_yaml": {
  "about": {
   "description": "During the exploration phase of a machine learning project, a data\nscientist tries to find the optimal pipeline for his specific use case.\nThis usually involves applying standard data cleaning steps, creating\nor selecting useful features, trying out different models, etc. Testing\nmultiple pipelines requires many lines of code, and writing it all in\nthe same notebook often makes it long and cluttered. On the other hand,\nusing multiple notebooks makes it harder to compare the results and to\nkeep an overview. On top of that, refactoring the code for every test\ncan be time-consuming. How many times have you conducted the same action\nto pre-process a raw dataset? How many times have you copy-and-pasted\ncode from an old repository to re-use it in a new use case?\n\nATOM is here to help solve these common issues. The package acts as\na wrapper of the whole machine learning pipeline, helping the data\nscientist to rapidly find a good model for his problem. Avoid\nendless imports and documentation lookups. Avoid rewriting the same\ncode over and over again. With just a few lines of code, it's now\npossible to perform basic data cleaning steps, select relevant\nfeatures and compare the performance of multiple models on a given\ndataset, providing quick insights on which pipeline performs best\nfor the task at hand.\n",
   "dev_url": "http://github.com/tvdboom/ATOM/tree/development",
   "doc_url": "https://tvdboom.github.io/ATOM/",
   "home": "http://github.com/tvdboom/ATOM",
   "license": "MIT",
   "license_file": "LICENSE",
   "summary": "A Python package for fast exploration of machine learning pipelines"
  },
  "build": {
   "noarch": "python",
   "number": "0",
   "script": "/usr/share/miniconda3/envs/run_env/conda-bld/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pla/bin/python -m pip install . --no-deps -vv"
  },
  "extra": {
   "recipe-maintainers": [
    "tvdboom"
   ]
  },
  "package": {
   "name": "atom-ml",
   "version": "4.8.0"
  },
  "requirements": {
   "host": [
    "python >=3.6",
    "pip"
   ],
   "run": [
    "python >=3.6",
    "numpy",
    "scipy",
    "pandas",
    "pandas-profiling",
    "mlflow",
    "dill",
    "tqdm",
    "joblib",
    "typeguard",
    "tabulate",
    "scikit-learn ~= 0.24.0",
    "scikit-optimize",
    "nltk",
    "tpot",
    "category_encoders",
    "imbalanced-learn",
    "featuretools",
    "gplearn",
    "matplotlib-base",
    "seaborn",
    "shap",
    "wordcloud"
   ]
  },
  "source": {
   "sha256": "0cd760cf0f48dbde17500f20bdb413fceef0e4247640b13f88627a87fbb74ee9",
   "url": "https://pypi.io/packages/source/a/atom-ml/atom-ml-4.8.0.tar.gz"
  },
  "test": {
   "commands": [
    "mkdir tests/files",
    "coverage run -m pytest"
   ],
   "imports": [
    "atom"
   ],
   "requires": [
    "pip",
    "pytest",
    "coverage",
    "python",
    "tensorflow",
    "numpy",
    "scipy",
    "pandas",
    "pandas-profiling",
    "mlflow",
    "dill",
    "tqdm",
    "joblib",
    "typeguard",
    "tabulate",
    "scikit-learn ~= 0.24.0",
    "scikit-optimize",
    "nltk",
    "tpot",
    "category_encoders",
    "imbalanced-learn",
    "featuretools",
    "gplearn",
    "matplotlib-base",
    "seaborn",
    "shap",
    "wordcloud",
    "py-xgboost",
    "lightgbm",
    "catboost"
   ],
   "source_files": [
    "tests"
   ]
  }
 },
 "name": "atom-ml",
 "new_version": "4.8.0",
 "new_version_attempts": {
  "4.2.0": 1,
  "4.3.0": 1,
  "4.7.3": 1
 },
 "new_version_errors": {},
 "outputs_names": {
  "__set__": true,
  "elements": [
   "atom-ml"
  ]
 },
 "pinning_version": "2021.09.11.17.45.07",
 "raw_meta_yaml": "{% set name = \"atom-ml\" %}\n{% set version = \"4.8.0\" %}\n\npackage:\n  name: {{ name|lower }}\n  version: {{ version }}\n\nsource:\n  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/{{ name }}-{{ version }}.tar.gz\n  sha256: 0cd760cf0f48dbde17500f20bdb413fceef0e4247640b13f88627a87fbb74ee9\n\n\nbuild:\n  number: 0\n  noarch: python\n  script: {{ PYTHON }} -m pip install . --no-deps -vv\n\nrequirements:\n  host:\n    - python >=3.6\n    - pip\n  run:\n    - python >=3.6\n    - numpy\n    - scipy\n    - pandas\n    - pandas-profiling\n    - mlflow\n    - dill\n    - tqdm\n    - joblib\n    - typeguard\n    - tabulate\n    - scikit-learn ~= 0.24.0\n    - scikit-optimize\n    - nltk\n    - tpot\n    - category_encoders\n    - imbalanced-learn\n    - featuretools\n    - gplearn\n    - matplotlib-base\n    - seaborn\n    - shap\n    - wordcloud\n\ntest:\n  requires:\n    - pip\n    - pytest\n    - coverage\n    - python\n    - tensorflow\n    - numpy\n    - scipy\n    - pandas\n    - pandas-profiling\n    - mlflow\n    - dill\n    - tqdm\n    - joblib\n    - typeguard\n    - tabulate\n    - scikit-learn ~= 0.24.0\n    - scikit-optimize\n    - nltk\n    - tpot\n    - category_encoders\n    - imbalanced-learn\n    - featuretools\n    - gplearn\n    - matplotlib-base\n    - seaborn\n    - shap\n    - wordcloud\n    - py-xgboost  # [not win]\n    - lightgbm\n    - catboost\n  source_files:\n    - tests\n  imports:\n    - atom\n  commands:\n    - mkdir tests/files  # Directory to save files during testing\n    - coverage run -m pytest\n\nabout:\n  home: http://github.com/tvdboom/ATOM\n  license: MIT\n  license_file: LICENSE\n  summary: A Python package for fast exploration of machine learning pipelines\n  description: |\n    During the exploration phase of a machine learning project, a data\n    scientist tries to find the optimal pipeline for his specific use case.\n    This usually involves applying standard data cleaning steps, creating\n    or selecting useful features, trying out different models, etc. Testing\n    multiple pipelines requires many lines of code, and writing it all in\n    the same notebook often makes it long and cluttered. On the other hand,\n    using multiple notebooks makes it harder to compare the results and to\n    keep an overview. On top of that, refactoring the code for every test\n    can be time-consuming. How many times have you conducted the same action\n    to pre-process a raw dataset? How many times have you copy-and-pasted\n    code from an old repository to re-use it in a new use case?\n\n    ATOM is here to help solve these common issues. The package acts as\n    a wrapper of the whole machine learning pipeline, helping the data\n    scientist to rapidly find a good model for his problem. Avoid\n    endless imports and documentation lookups. Avoid rewriting the same\n    code over and over again. With just a few lines of code, it's now\n    possible to perform basic data cleaning steps, select relevant\n    features and compare the performance of multiple models on a given\n    dataset, providing quick insights on which pipeline performs best\n    for the task at hand.\n\n  doc_url: https://tvdboom.github.io/ATOM/\n  dev_url: http://github.com/tvdboom/ATOM/tree/development\n\nextra:\n  recipe-maintainers:\n    - tvdboom\n",
 "req": {
  "__set__": true,
  "elements": [
   "category_encoders",
   "dill",
   "featuretools",
   "gplearn",
   "imbalanced-learn",
   "joblib",
   "matplotlib-base",
   "mlflow",
   "nltk",
   "numpy",
   "pandas",
   "pandas-profiling",
   "pip",
   "python",
   "scikit-learn",
   "scikit-optimize",
   "scipy",
   "seaborn",
   "shap",
   "tabulate",
   "tpot",
   "tqdm",
   "typeguard",
   "wordcloud"
  ]
 },
 "requirements": {
  "build": {
   "__set__": true,
   "elements": []
  },
  "host": {
   "__set__": true,
   "elements": [
    "pip",
    "python"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "category_encoders",
    "dill",
    "featuretools",
    "gplearn",
    "imbalanced-learn",
    "joblib",
    "matplotlib-base",
    "mlflow",
    "nltk",
    "numpy",
    "pandas",
    "pandas-profiling",
    "python",
    "scikit-learn",
    "scikit-optimize",
    "scipy",
    "seaborn",
    "shap",
    "tabulate",
    "tpot",
    "tqdm",
    "typeguard",
    "wordcloud"
   ]
  },
  "test": {
   "__set__": true,
   "elements": [
    "catboost",
    "category_encoders",
    "coverage",
    "dill",
    "featuretools",
    "gplearn",
    "imbalanced-learn",
    "joblib",
    "lightgbm",
    "matplotlib-base",
    "mlflow",
    "nltk",
    "numpy",
    "pandas",
    "pandas-profiling",
    "pip",
    "py-xgboost",
    "pytest",
    "python",
    "scikit-learn",
    "scikit-optimize",
    "scipy",
    "seaborn",
    "shap",
    "tabulate",
    "tensorflow",
    "tpot",
    "tqdm",
    "typeguard",
    "wordcloud"
   ]
  }
 },
 "smithy_version": "3.12",
 "strong_exports": false,
 "total_requirements": {
  "build": {
   "__set__": true,
   "elements": []
  },
  "host": {
   "__set__": true,
   "elements": [
    "pip",
    "python >=3.6"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "category_encoders",
    "dill",
    "featuretools",
    "gplearn",
    "imbalanced-learn",
    "joblib",
    "matplotlib-base",
    "mlflow",
    "nltk",
    "numpy",
    "pandas",
    "pandas-profiling",
    "python >=3.6",
    "scikit-learn ~= 0.24.0",
    "scikit-optimize",
    "scipy",
    "seaborn",
    "shap",
    "tabulate",
    "tpot",
    "tqdm",
    "typeguard",
    "wordcloud"
   ]
  },
  "test": {
   "__set__": true,
   "elements": [
    "catboost",
    "category_encoders",
    "coverage",
    "dill",
    "featuretools",
    "gplearn",
    "imbalanced-learn",
    "joblib",
    "lightgbm",
    "matplotlib-base",
    "mlflow",
    "nltk",
    "numpy",
    "pandas",
    "pandas-profiling",
    "pip",
    "py-xgboost",
    "pytest",
    "python",
    "scikit-learn ~= 0.24.0",
    "scikit-optimize",
    "scipy",
    "seaborn",
    "shap",
    "tabulate",
    "tensorflow",
    "tpot",
    "tqdm",
    "typeguard",
    "wordcloud"
   ]
  }
 },
 "url": "https://pypi.io/packages/source/a/atom-ml/atom-ml-4.8.0.tar.gz",
 "version": "4.8.0"
}