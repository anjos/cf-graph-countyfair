{
 "archived": false,
 "bad": false,
 "branch": "master",
 "conda-forge.yml": {},
 "feedstock_name": "atom-ml",
 "hash_type": "sha256",
 "linux_64_meta_yaml": {
  "about": {
   "description": "There is no magic formula in data science that can tell us which type of machine\nlearning algorithm will perform best for a specific use-case. Different models\nare better suited for different types of data and different problems. At best,\nyou can follow some rough guide on how to approach problems with regard to which\nmodel to try on your data, but these are often more confusing than helpful. Best\npractices tell us to start with a simple model (e.g. linear regression) and build\nup to more complicated models (e.g. logistic regression -> random forest ->\nmulti-layer perceptron) if you are not satisfied with the results. Unfortunately,\ndifferent models require different data cleaning steps, different type/amount of\nfeatures, tuning a new set of hyperparameters, etc. Refactoring the code for this\npurpose can be quite boring and time consuming. Because of this, many data\nscientists end up just using the model best known to them and fine-tuning this\nparticular model without ever trying different ones. This can result in poor\nperformance (because the model is just not the right one for the task) or in poor\ntime management (because you could have achieved a similar performance with a\nsimpler/faster model).\n\nATOM is here to help us solve these issues. With just a few lines of code, you\ncan perform basic data cleaning steps, select relevant features and compare the\nperformance of multiple models on a given dataset. ATOM should be able to provide\nquick insights on which algorithms perform best for the task at hand and provide\nan indication of the feasibility of the ML solution.\n\nIt is important to realize that ATOM is not here to replace all the work a data\nscientist has to do before getting his model into production. ATOM doesn't spit\nout production-ready models just by tuning some parameters in its API. After\nhelping you to determine the right model, you will most probably need to fine-tune\nit using use-case specific features and data cleaning steps in order to achieve\nmaximum performance.\n\nSo, this sounds a bit like AutoML, how is ATOM different than auto-sklearn or\nTPOT? Well, ATOM does AutoML in the sense that it helps you find the best model\nfor a specific task, but contrary to the aforementioned packages, it does not\nactively search for the best model. It simply runs all of them and let you pick\nthe one that you think suites you best. AutoML packages are often black boxes: if\nyou provide data, it will magically return a working model. Although it works\ngreat, they often produce complicated pipelines with low explainability, hard to\nsell to the business. In this, ATOM excels. Every step of the pipeline is\naccounted for, and using the provided plotting methods, it\u2019s easy to demonstrate\nwhy a model is better/worse than the other.\n",
   "dev_url": "http://github.com/tvdboom/ATOM/tree/development",
   "doc_source_url": "http://github.com/tvdboom/ATOM/docs_sources",
   "doc_url": "http://github.com/tvdboom/ATOM/docs",
   "home": "http://github.com/tvdboom/ATOM",
   "license": "MIT",
   "license_file": "LICENSE",
   "summary": "A Python AutoML tool for fast exploration and experimentation of supervised machine learning pipelines."
  },
  "build": {
   "noarch": "python",
   "number": "0",
   "script": "-m pip install . --no-deps -vv"
  },
  "extra": {
   "recipe-maintainers": [
    "tvdboom"
   ]
  },
  "package": {
   "name": "atom-ml",
   "version": "4.1.0"
  },
  "requirements": {
   "host": [
    "python >=3.6",
    "pip"
   ],
   "run": [
    "python >=3.6",
    "numpy",
    "scipy",
    "pandas",
    "tqdm",
    "joblib",
    "typeguard",
    "tabulate",
    "scikit-learn",
    "scikit-optimize",
    "category_encoders",
    "imbalanced-learn",
    "pandas-profiling",
    "featuretools",
    "gplearn",
    "matplotlib-base",
    "seaborn",
    "shap"
   ]
  },
  "source": {
   "sha256": "3c2ff5b60a92014cde6d858fb3b9740aa27ce3edc88c4d94a852c79ab6bf20fb",
   "url": "https://pypi.io/packages/source/a/atom-ml/atom-ml-4.1.0.tar.gz"
  },
  "test": {
   "commands": [
    "mkdir tests/files",
    "coverage run -m pytest"
   ],
   "imports": [
    "atom"
   ],
   "requires": [
    "pip",
    "pytest",
    "coverage",
    "python",
    "numpy",
    "scipy",
    "pandas",
    "tqdm",
    "joblib",
    "typeguard",
    "tabulate",
    "scikit-learn",
    "scikit-optimize",
    "category_encoders",
    "imbalanced-learn",
    "pandas-profiling",
    "featuretools",
    "gplearn",
    "matplotlib-base",
    "seaborn",
    "shap",
    "py-xgboost",
    "lightgbm",
    "catboost"
   ],
   "source_files": [
    "tests"
   ]
  }
 },
 "linux_64_requirements": {
  "build": {
   "__set__": true,
   "elements": []
  },
  "host": {
   "__set__": true,
   "elements": [
    "pip",
    "python"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "category_encoders",
    "featuretools",
    "gplearn",
    "imbalanced-learn",
    "joblib",
    "matplotlib-base",
    "numpy",
    "pandas",
    "pandas-profiling",
    "python",
    "scikit-learn",
    "scikit-optimize",
    "scipy",
    "seaborn",
    "shap",
    "tabulate",
    "tqdm",
    "typeguard"
   ]
  },
  "test": {
   "__set__": true,
   "elements": [
    "catboost",
    "category_encoders",
    "coverage",
    "featuretools",
    "gplearn",
    "imbalanced-learn",
    "joblib",
    "lightgbm",
    "matplotlib-base",
    "numpy",
    "pandas",
    "pandas-profiling",
    "pip",
    "py-xgboost",
    "pytest",
    "python",
    "scikit-learn",
    "scikit-optimize",
    "scipy",
    "seaborn",
    "shap",
    "tabulate",
    "tqdm",
    "typeguard"
   ]
  }
 },
 "meta_yaml": {
  "about": {
   "description": "There is no magic formula in data science that can tell us which type of machine\nlearning algorithm will perform best for a specific use-case. Different models\nare better suited for different types of data and different problems. At best,\nyou can follow some rough guide on how to approach problems with regard to which\nmodel to try on your data, but these are often more confusing than helpful. Best\npractices tell us to start with a simple model (e.g. linear regression) and build\nup to more complicated models (e.g. logistic regression -> random forest ->\nmulti-layer perceptron) if you are not satisfied with the results. Unfortunately,\ndifferent models require different data cleaning steps, different type/amount of\nfeatures, tuning a new set of hyperparameters, etc. Refactoring the code for this\npurpose can be quite boring and time consuming. Because of this, many data\nscientists end up just using the model best known to them and fine-tuning this\nparticular model without ever trying different ones. This can result in poor\nperformance (because the model is just not the right one for the task) or in poor\ntime management (because you could have achieved a similar performance with a\nsimpler/faster model).\n\nATOM is here to help us solve these issues. With just a few lines of code, you\ncan perform basic data cleaning steps, select relevant features and compare the\nperformance of multiple models on a given dataset. ATOM should be able to provide\nquick insights on which algorithms perform best for the task at hand and provide\nan indication of the feasibility of the ML solution.\n\nIt is important to realize that ATOM is not here to replace all the work a data\nscientist has to do before getting his model into production. ATOM doesn't spit\nout production-ready models just by tuning some parameters in its API. After\nhelping you to determine the right model, you will most probably need to fine-tune\nit using use-case specific features and data cleaning steps in order to achieve\nmaximum performance.\n\nSo, this sounds a bit like AutoML, how is ATOM different than auto-sklearn or\nTPOT? Well, ATOM does AutoML in the sense that it helps you find the best model\nfor a specific task, but contrary to the aforementioned packages, it does not\nactively search for the best model. It simply runs all of them and let you pick\nthe one that you think suites you best. AutoML packages are often black boxes: if\nyou provide data, it will magically return a working model. Although it works\ngreat, they often produce complicated pipelines with low explainability, hard to\nsell to the business. In this, ATOM excels. Every step of the pipeline is\naccounted for, and using the provided plotting methods, it\u2019s easy to demonstrate\nwhy a model is better/worse than the other.\n",
   "dev_url": "http://github.com/tvdboom/ATOM/tree/development",
   "doc_source_url": "http://github.com/tvdboom/ATOM/docs_sources",
   "doc_url": "http://github.com/tvdboom/ATOM/docs",
   "home": "http://github.com/tvdboom/ATOM",
   "license": "MIT",
   "license_file": "LICENSE",
   "summary": "A Python AutoML tool for fast exploration and experimentation of supervised machine learning pipelines."
  },
  "build": {
   "noarch": "python",
   "number": "0",
   "script": "-m pip install . --no-deps -vv"
  },
  "extra": {
   "recipe-maintainers": [
    "tvdboom"
   ]
  },
  "package": {
   "name": "atom-ml",
   "version": "4.1.0"
  },
  "requirements": {
   "host": [
    "python >=3.6",
    "pip"
   ],
   "run": [
    "python >=3.6",
    "numpy",
    "scipy",
    "pandas",
    "tqdm",
    "joblib",
    "typeguard",
    "tabulate",
    "scikit-learn",
    "scikit-optimize",
    "category_encoders",
    "imbalanced-learn",
    "pandas-profiling",
    "featuretools",
    "gplearn",
    "matplotlib-base",
    "seaborn",
    "shap"
   ]
  },
  "source": {
   "sha256": "3c2ff5b60a92014cde6d858fb3b9740aa27ce3edc88c4d94a852c79ab6bf20fb",
   "url": "https://pypi.io/packages/source/a/atom-ml/atom-ml-4.1.0.tar.gz"
  },
  "test": {
   "commands": [
    "mkdir tests/files",
    "coverage run -m pytest"
   ],
   "imports": [
    "atom"
   ],
   "requires": [
    "pip",
    "pytest",
    "coverage",
    "python",
    "numpy",
    "scipy",
    "pandas",
    "tqdm",
    "joblib",
    "typeguard",
    "tabulate",
    "scikit-learn",
    "scikit-optimize",
    "category_encoders",
    "imbalanced-learn",
    "pandas-profiling",
    "featuretools",
    "gplearn",
    "matplotlib-base",
    "seaborn",
    "shap",
    "py-xgboost",
    "lightgbm",
    "catboost"
   ],
   "source_files": [
    "tests"
   ]
  }
 },
 "name": "atom-ml",
 "new_version": "4.1.0",
 "outputs_names": {
  "__set__": true,
  "elements": [
   "atom-ml"
  ]
 },
 "raw_meta_yaml": "{% set name = \"atom-ml\" %}\n{% set version = \"4.1.0\" %}\n\npackage:\n  name: {{ name|lower }}\n  version: {{ version }}\n\nsource:\n  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/{{ name }}-{{ version }}.tar.gz\n  sha256: 3c2ff5b60a92014cde6d858fb3b9740aa27ce3edc88c4d94a852c79ab6bf20fb\n\nbuild:\n  number: 0\n  noarch: python\n  script: {{ PYTHON }} -m pip install . --no-deps -vv\n\nrequirements:\n  host:\n    - python >=3.6\n    - pip\n  run:\n    - python >=3.6\n    - numpy\n    - scipy\n    - pandas\n    - tqdm\n    - joblib\n    - typeguard\n    - tabulate\n    - scikit-learn\n    - scikit-optimize\n    - category_encoders\n    - imbalanced-learn\n    - pandas-profiling\n    - featuretools\n    - gplearn\n    - matplotlib-base\n    - seaborn\n    - shap\n\ntest:\n  requires:\n    - pip\n    - pytest\n    - coverage\n    - python\n    - numpy\n    - scipy\n    - pandas\n    - tqdm\n    - joblib\n    - typeguard\n    - tabulate\n    - scikit-learn\n    - scikit-optimize\n    - category_encoders\n    - imbalanced-learn\n    - pandas-profiling\n    - featuretools\n    - gplearn\n    - matplotlib-base\n    - seaborn\n    - shap\n    - py-xgboost  # [not win]\n    - lightgbm\n    - catboost\n  source_files:\n    - tests\n  imports:\n    - atom\n  commands:\n    - mkdir tests/files  # Directory to save files during testing\n    - coverage run -m pytest\n\nabout:\n  home: http://github.com/tvdboom/ATOM\n  license: MIT\n  license_file: LICENSE\n  summary: A Python AutoML tool for fast exploration and experimentation of supervised machine learning pipelines.\n  description: |\n    There is no magic formula in data science that can tell us which type of machine\n    learning algorithm will perform best for a specific use-case. Different models\n    are better suited for different types of data and different problems. At best,\n    you can follow some rough guide on how to approach problems with regard to which\n    model to try on your data, but these are often more confusing than helpful. Best\n    practices tell us to start with a simple model (e.g. linear regression) and build\n    up to more complicated models (e.g. logistic regression -> random forest ->\n    multi-layer perceptron) if you are not satisfied with the results. Unfortunately,\n    different models require different data cleaning steps, different type/amount of\n    features, tuning a new set of hyperparameters, etc. Refactoring the code for this\n    purpose can be quite boring and time consuming. Because of this, many data\n    scientists end up just using the model best known to them and fine-tuning this\n    particular model without ever trying different ones. This can result in poor\n    performance (because the model is just not the right one for the task) or in poor\n    time management (because you could have achieved a similar performance with a\n    simpler/faster model).\n\n    ATOM is here to help us solve these issues. With just a few lines of code, you\n    can perform basic data cleaning steps, select relevant features and compare the\n    performance of multiple models on a given dataset. ATOM should be able to provide\n    quick insights on which algorithms perform best for the task at hand and provide\n    an indication of the feasibility of the ML solution.\n\n    It is important to realize that ATOM is not here to replace all the work a data\n    scientist has to do before getting his model into production. ATOM doesn't spit\n    out production-ready models just by tuning some parameters in its API. After\n    helping you to determine the right model, you will most probably need to fine-tune\n    it using use-case specific features and data cleaning steps in order to achieve\n    maximum performance.\n\n    So, this sounds a bit like AutoML, how is ATOM different than auto-sklearn or\n    TPOT? Well, ATOM does AutoML in the sense that it helps you find the best model\n    for a specific task, but contrary to the aforementioned packages, it does not\n    actively search for the best model. It simply runs all of them and let you pick\n    the one that you think suites you best. AutoML packages are often black boxes: if\n    you provide data, it will magically return a working model. Although it works\n    great, they often produce complicated pipelines with low explainability, hard to\n    sell to the business. In this, ATOM excels. Every step of the pipeline is\n    accounted for, and using the provided plotting methods, it\u2019s easy to demonstrate\n    why a model is better/worse than the other.\n\n  doc_url: http://github.com/tvdboom/ATOM/docs\n  doc_source_url: http://github.com/tvdboom/ATOM/docs_sources\n  dev_url: http://github.com/tvdboom/ATOM/tree/development\n\nextra:\n  recipe-maintainers:\n    - tvdboom\n",
 "req": {
  "__set__": true,
  "elements": [
   "category_encoders",
   "featuretools",
   "gplearn",
   "imbalanced-learn",
   "joblib",
   "matplotlib-base",
   "numpy",
   "pandas",
   "pandas-profiling",
   "pip",
   "python",
   "scikit-learn",
   "scikit-optimize",
   "scipy",
   "seaborn",
   "shap",
   "tabulate",
   "tqdm",
   "typeguard"
  ]
 },
 "requirements": {
  "build": {
   "__set__": true,
   "elements": []
  },
  "host": {
   "__set__": true,
   "elements": [
    "pip",
    "python"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "category_encoders",
    "featuretools",
    "gplearn",
    "imbalanced-learn",
    "joblib",
    "matplotlib-base",
    "numpy",
    "pandas",
    "pandas-profiling",
    "python",
    "scikit-learn",
    "scikit-optimize",
    "scipy",
    "seaborn",
    "shap",
    "tabulate",
    "tqdm",
    "typeguard"
   ]
  },
  "test": {
   "__set__": true,
   "elements": [
    "catboost",
    "category_encoders",
    "coverage",
    "featuretools",
    "gplearn",
    "imbalanced-learn",
    "joblib",
    "lightgbm",
    "matplotlib-base",
    "numpy",
    "pandas",
    "pandas-profiling",
    "pip",
    "py-xgboost",
    "pytest",
    "python",
    "scikit-learn",
    "scikit-optimize",
    "scipy",
    "seaborn",
    "shap",
    "tabulate",
    "tqdm",
    "typeguard"
   ]
  }
 },
 "strong_exports": false,
 "total_requirements": {
  "build": {
   "__set__": true,
   "elements": []
  },
  "host": {
   "__set__": true,
   "elements": [
    "pip",
    "python >=3.6"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "category_encoders",
    "featuretools",
    "gplearn",
    "imbalanced-learn",
    "joblib",
    "matplotlib-base",
    "numpy",
    "pandas",
    "pandas-profiling",
    "python >=3.6",
    "scikit-learn",
    "scikit-optimize",
    "scipy",
    "seaborn",
    "shap",
    "tabulate",
    "tqdm",
    "typeguard"
   ]
  },
  "test": {
   "__set__": true,
   "elements": [
    "catboost",
    "category_encoders",
    "coverage",
    "featuretools",
    "gplearn",
    "imbalanced-learn",
    "joblib",
    "lightgbm",
    "matplotlib-base",
    "numpy",
    "pandas",
    "pandas-profiling",
    "pip",
    "py-xgboost",
    "pytest",
    "python",
    "scikit-learn",
    "scikit-optimize",
    "scipy",
    "seaborn",
    "shap",
    "tabulate",
    "tqdm",
    "typeguard"
   ]
  }
 },
 "url": "https://pypi.io/packages/source/a/atom-ml/atom-ml-4.1.0.tar.gz",
 "version": "4.1.0"
}