{
 "PRed": [
  {
   "PR": {
    "__lazy_json__": "pr_json/367072026.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "Version",
    "migrator_version": 0,
    "version": "2.5.7-1"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "version"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/394864347.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "Version",
    "migrator_version": 0,
    "version": "2.6.4-1"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "version"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/438263516.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "Version",
    "migrator_version": 0,
    "version": "2.7.3-1"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "version"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/440809195.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "Version",
    "migrator_version": 0,
    "version": "2.7.5-1"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "version"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/440932322.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "Version",
    "migrator_version": 0,
    "version": "2.7.6-1"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "version"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/457469808.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "Version",
    "migrator_version": 0,
    "version": "2.7.8-1"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "version"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/499502331.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "MigrationYaml",
    "migrator_object_version": 1,
    "migrator_version": 0,
    "name": "cuda110"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_object_version",
    "migrator_version",
    "name"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/522742589.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "Version",
    "migrator_version": 0,
    "version": "2.8.3-1"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "version"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/ba58a310-e3a2-49de-98e4-b53707eff534.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "MigrationYaml",
    "migrator_object_version": 1,
    "migrator_version": 0,
    "name": "windows_cuda"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_object_version",
    "migrator_version",
    "name"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/542193006.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "ArchRebuild",
    "migrator_version": 1,
    "name": "aarch64 and ppc64le addition"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "name"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/542239902.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "OSXArm",
    "migrator_version": 1,
    "name": "arm osx addition"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "name"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/570739377.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "Version",
    "migrator_version": 0,
    "version": "2.8.4-1"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "version"
   ]
  }
 ],
 "archived": false,
 "bad": false,
 "branch": "master",
 "conda-forge.yml": {},
 "feedstock_name": "nccl",
 "hash_type": "sha256",
 "linux_64_meta_yaml": {
  "about": {
   "description": "The NVIDIA Collective Communications Library (NCCL) implements multi-GPU\nand multi-node collective communication primitives that are performance\noptimized for NVIDIA GPUs. NCCL provides routines such as all-gather,\nall-reduce, broadcast, reduce, reduce-scatter, that are optimized to\nachieve high bandwidth over PCIe and NVLink high-speed interconnect.\n",
   "dev_url": "https://github.com/NVIDIA/nccl",
   "doc_url": "https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/index.html",
   "home": "https://developer.nvidia.com/nccl",
   "license": "BSD-3-Clause",
   "license_family": "BSD",
   "license_file": "LICENSE.txt",
   "summary": "Optimized primitives for collective multi-GPU communication"
  },
  "build": {
   "number": "0",
   "run_exports": [
    "nccl",
    "nccl",
    "nccl",
    "nccl",
    "nccl"
   ]
  },
  "extra": {
   "recipe-maintainers": [
    "jakirkham",
    "jakirkham",
    "jakirkham",
    "jakirkham",
    "jakirkham"
   ]
  },
  "package": {
   "name": "nccl",
   "version": "2.8.3.1"
  },
  "requirements": {
   "build": [
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "make",
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "make",
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "make",
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "make",
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "make"
   ]
  },
  "source": {
   "sha256": "3ae89ddb2956fff081e406a94ff54ae5e52359f5d645ce977c7eba09b3b782e6",
   "url": "https://github.com/NVIDIA/nccl/archive/v2.8.3-1.tar.gz"
  },
  "test": {
   "commands": [
    "test -f \"${PREFIX}/include/nccl.h\"",
    "test -f \"${PREFIX}/lib/libnccl.so\"",
    "test -f \"${PREFIX}/lib/libnccl_static.a\"",
    "test -f \"${PREFIX}/include/nccl.h\"",
    "test -f \"${PREFIX}/lib/libnccl.so\"",
    "test -f \"${PREFIX}/lib/libnccl_static.a\"",
    "test -f \"${PREFIX}/include/nccl.h\"",
    "test -f \"${PREFIX}/lib/libnccl.so\"",
    "test -f \"${PREFIX}/lib/libnccl_static.a\"",
    "test -f \"${PREFIX}/include/nccl.h\"",
    "test -f \"${PREFIX}/lib/libnccl.so\"",
    "test -f \"${PREFIX}/lib/libnccl_static.a\"",
    "test -f \"${PREFIX}/include/nccl.h\"",
    "test -f \"${PREFIX}/lib/libnccl.so\"",
    "test -f \"${PREFIX}/lib/libnccl_static.a\""
   ]
  }
 },
 "linux_64_requirements": {
  "build": {
   "__set__": true,
   "elements": [
    "c_compiler_stub",
    "cuda_compiler_stub",
    "cxx_compiler_stub",
    "make"
   ]
  },
  "host": {
   "__set__": true,
   "elements": []
  },
  "run": {
   "__set__": true,
   "elements": []
  },
  "test": {
   "__set__": true,
   "elements": []
  }
 },
 "meta_yaml": {
  "about": {
   "description": "The NVIDIA Collective Communications Library (NCCL) implements multi-GPU\nand multi-node collective communication primitives that are performance\noptimized for NVIDIA GPUs. NCCL provides routines such as all-gather,\nall-reduce, broadcast, reduce, reduce-scatter, that are optimized to\nachieve high bandwidth over PCIe and NVLink high-speed interconnect.\n",
   "dev_url": "https://github.com/NVIDIA/nccl",
   "doc_url": "https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/index.html",
   "home": "https://developer.nvidia.com/nccl",
   "license": "BSD-3-Clause",
   "license_family": "BSD",
   "license_file": "LICENSE.txt",
   "summary": "Optimized primitives for collective multi-GPU communication"
  },
  "build": {
   "number": "0",
   "run_exports": [
    "nccl",
    "nccl",
    "nccl",
    "nccl",
    "nccl"
   ]
  },
  "extra": {
   "recipe-maintainers": [
    "jakirkham",
    "jakirkham",
    "jakirkham",
    "jakirkham",
    "jakirkham"
   ]
  },
  "package": {
   "name": "nccl",
   "version": "2.8.3.1"
  },
  "requirements": {
   "build": [
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "make",
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "make",
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "make",
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "make",
    "c_compiler_stub",
    "cxx_compiler_stub",
    "cuda_compiler_stub",
    "make"
   ]
  },
  "source": {
   "sha256": "3ae89ddb2956fff081e406a94ff54ae5e52359f5d645ce977c7eba09b3b782e6",
   "url": "https://github.com/NVIDIA/nccl/archive/v2.8.3-1.tar.gz"
  },
  "test": {
   "commands": [
    "test -f \"${PREFIX}/include/nccl.h\"",
    "test -f \"${PREFIX}/lib/libnccl.so\"",
    "test -f \"${PREFIX}/lib/libnccl_static.a\"",
    "test -f \"${PREFIX}/include/nccl.h\"",
    "test -f \"${PREFIX}/lib/libnccl.so\"",
    "test -f \"${PREFIX}/lib/libnccl_static.a\"",
    "test -f \"${PREFIX}/include/nccl.h\"",
    "test -f \"${PREFIX}/lib/libnccl.so\"",
    "test -f \"${PREFIX}/lib/libnccl_static.a\"",
    "test -f \"${PREFIX}/include/nccl.h\"",
    "test -f \"${PREFIX}/lib/libnccl.so\"",
    "test -f \"${PREFIX}/lib/libnccl_static.a\"",
    "test -f \"${PREFIX}/include/nccl.h\"",
    "test -f \"${PREFIX}/lib/libnccl.so\"",
    "test -f \"${PREFIX}/lib/libnccl_static.a\""
   ]
  }
 },
 "name": "nccl",
 "new_version": "2.8.4-1",
 "new_version_attempts": {
  "2.7.3-1": 1,
  "2.7.5-1": 1,
  "2.7.6-1": 1,
  "2.7.8-1": 1,
  "2.8.3-1": 1,
  "2.8.4-1": 1
 },
 "new_version_errors": {},
 "outputs_names": {
  "__set__": true,
  "elements": [
   "nccl"
  ]
 },
 "pinning_version": "2021.02.09.20.54.49",
 "raw_meta_yaml": "{% set name = \"nccl\" %}\n{% set version = \"2.8.3-1\" %}\n\npackage:\n  name: {{ name|lower }}\n  version: {{ version|replace(\"-\", \".\") }}\n\nsource:\n  url: https://github.com/NVIDIA/nccl/archive/v{{ version }}.tar.gz\n  sha256: 3ae89ddb2956fff081e406a94ff54ae5e52359f5d645ce977c7eba09b3b782e6\n\nbuild:\n  number: 0\n  skip: true  # [(not linux64) or (cuda_compiler_version == \"None\")]\n  run_exports:\n    # xref: https://github.com/NVIDIA/nccl/issues/218\n    - {{ pin_subpackage(name, max_pin=\"x\") }}\n\nrequirements:\n  build:\n    - {{ compiler(\"c\") }}\n    - {{ compiler(\"cxx\") }}\n    - {{ compiler(\"cuda\") }}\n    - make\n\ntest:\n  commands:\n    - test -f \"${PREFIX}/include/nccl.h\"\n    - test -f \"${PREFIX}/lib/libnccl.so\"\n    - test -f \"${PREFIX}/lib/libnccl_static.a\"\n\nabout:\n  home: https://developer.nvidia.com/nccl\n  license: BSD-3-Clause\n  license_family: BSD\n  license_file: LICENSE.txt\n  summary: Optimized primitives for collective multi-GPU communication\n\n  description: |\n    The NVIDIA Collective Communications Library (NCCL) implements multi-GPU\n    and multi-node collective communication primitives that are performance\n    optimized for NVIDIA GPUs. NCCL provides routines such as all-gather,\n    all-reduce, broadcast, reduce, reduce-scatter, that are optimized to\n    achieve high bandwidth over PCIe and NVLink high-speed interconnect.\n\n  doc_url: https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/index.html\n  dev_url: https://github.com/NVIDIA/nccl\n\nextra:\n  recipe-maintainers:\n    - jakirkham\n",
 "req": {
  "__set__": true,
  "elements": [
   "c_compiler_stub",
   "cuda_compiler_stub",
   "cxx_compiler_stub",
   "make"
  ]
 },
 "requirements": {
  "build": {
   "__set__": true,
   "elements": [
    "c_compiler_stub",
    "cuda_compiler_stub",
    "cxx_compiler_stub",
    "make"
   ]
  },
  "host": {
   "__set__": true,
   "elements": [
    "c_compiler_stub",
    "cuda_compiler_stub",
    "cxx_compiler_stub"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "c_compiler_stub",
    "cuda_compiler_stub",
    "cxx_compiler_stub"
   ]
  },
  "test": {
   "__set__": true,
   "elements": []
  }
 },
 "smithy_version": "3.8.6",
 "strong_exports": false,
 "total_requirements": {
  "build": {
   "__set__": true,
   "elements": [
    "c_compiler_stub",
    "cuda_compiler_stub",
    "cxx_compiler_stub",
    "make"
   ]
  },
  "host": {
   "__set__": true,
   "elements": []
  },
  "run": {
   "__set__": true,
   "elements": []
  },
  "test": {
   "__set__": true,
   "elements": []
  }
 },
 "url": "https://github.com/NVIDIA/nccl/archive/v2.8.3-1.tar.gz",
 "version": "2.8.3.1"
}