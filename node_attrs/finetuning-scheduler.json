{
 "archived": false,
 "bad": "make_graph: render error No module named 'toml'\nTraceback (most recent call last):\n  File \"/home/runner/work/autotick-bot/autotick-bot/cf-scripts/conda_forge_tick/feedstock_parser.py\", line 241, in populate_feedstock_attributes\n    parse_meta_yaml(\n  File \"/home/runner/work/autotick-bot/autotick-bot/cf-scripts/conda_forge_tick/utils.py\", line 167, in parse_meta_yaml\n    return _parse_meta_yaml_impl(\n  File \"/home/runner/work/autotick-bot/autotick-bot/cf-scripts/conda_forge_tick/utils.py\", line 239, in _parse_meta_yaml_impl\n    m = MetaData(tmpdir, config=config, variant=var)\n  File \"/usr/share/miniconda3/envs/run_env/lib/python3.9/site-packages/conda_build/metadata.py\", line 932, in __init__\n    self.parse_again(permit_undefined_jinja=True, allow_no_other_outputs=True)\n  File \"/usr/share/miniconda3/envs/run_env/lib/python3.9/site-packages/conda_build/metadata.py\", line 1007, in parse_again\n    self.meta = parse(self._get_contents(permit_undefined_jinja,\n  File \"/usr/share/miniconda3/envs/run_env/lib/python3.9/site-packages/conda_build/metadata.py\", line 1546, in _get_contents\n    from conda_build.jinja_context import context_processor, UndefinedNeverFail, FilteredLoader\n  File \"/usr/share/miniconda3/envs/run_env/lib/python3.9/site-packages/conda_build/jinja_context.py\", line 13, in <module>\n    import toml\nModuleNotFoundError: No module named 'toml'\n",
 "branch": "main",
 "conda-forge.yml": {},
 "feedstock_name": "finetuning-scheduler",
 "hash_type": "sha256",
 "name": "finetuning-scheduler",
 "new_version": "0.2.0",
 "outputs_names": {
  "__set__": true,
  "elements": [
   "finetuning-scheduler"
  ]
 },
 "raw_meta_yaml": "{% set name = \"finetuning-scheduler\" %}\n{% set version = \"0.2.0\" %}\n\npackage:\n  name: {{ name|lower }}\n  version: {{ version }}\n\nsource:\n  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/finetuning-scheduler-{{ version }}.tar.gz\n  sha256: 46dc8b71a2622eddad843d52ef1125a889aca71fa77d819d131f2993bf850e23\n\nbuild:\n  number: 0\n  noarch: python\n  script: {{ PYTHON }} -m pip install . -vv\n\nrequirements:\n  host:\n    - pip\n    - python >=3.7\n  run:\n    - python >=3.7\n    - pytorch-lightning >=1.7.0, <1.7.1\n    - pytorch >=1.9\n\ntest:\n  imports:\n    - finetuning_scheduler\n  requires:\n    - pip\n\nabout:\n  home: https://github.com/speediedan/finetuning-scheduler\n  summary: A PyTorch Lightning extension that enhances model experimentation with flexible fine-tuning schedules.\n  license: Apache-2.0\n  license_file: LICENSE\n  description: |\n    The FinetuningScheduler callback accelerates and enhances foundational model experimentation with flexible fine-tuning\n    schedules. Training with the FinetuningScheduler callback is simple and confers a host of benefits:\n\n    - it dramatically increases fine-tuning flexibility\n    - expedites and facilitates exploration of model tuning dynamics\n    - enables marginal performance improvements of finetuned models\n\n    Fundamentally, the FinetuningScheduler callback enables multi-phase, scheduled fine-tuning of foundational models.\n    Gradual unfreezing (i.e. thawing) can help maximize foundational model knowledge retention while allowing (typically\n    upper layers of) the model to optimally adapt to new tasks during transfer learning.\n\n    FinetuningScheduler orchestrates the gradual unfreezing of models via a fine-tuning schedule that is either implicitly\n    generated (the default) or explicitly provided by the user (more computationally efficient). Fine-tuning phase\n    transitions are driven by FTSEarlyStopping criteria (a multi-phase extension of EarlyStopping), user-specified epoch\n    transitions or a composition of the two (the default mode). A FinetuningScheduler training session completes when the\n    final phase of the schedule has its stopping criteria met.\n\n    Documentation\n    -------------\n    - https://finetuning-scheduler.readthedocs.io/en/stable/\n    - https://finetuning-scheduler.readthedocs.io/en/0.2.0/\n\nextra:\n  recipe-maintainers:\n    - speediedan\n",
 "strong_exports": false,
 "url": "https://pypi.io/packages/source/f/finetuning-scheduler/finetuning-scheduler-0.2.0.tar.gz",
 "version": "0.2.0"
}