{
 "PRed": [
  {
   "PR": {
    "__lazy_json__": "pr_json/447105311.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "Version",
    "migrator_version": 0,
    "version": "0.1.92"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "version"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/447106698.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "MigrationYaml",
    "migrator_object_version": 1,
    "migrator_version": 0,
    "name": "python38"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_object_version",
    "migrator_version",
    "name"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/447108933.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "MigrationYaml",
    "migrator_object_version": 1,
    "migrator_version": 0,
    "name": "pypy"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_object_version",
    "migrator_version",
    "name"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/453211476.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "Version",
    "migrator_version": 0,
    "version": "1.0.0"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "version"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/500924376.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "MigrationYaml",
    "migrator_object_version": 2,
    "migrator_version": 0,
    "name": "python39"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_object_version",
    "migrator_version",
    "name"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/551763134.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "MigrationYaml",
    "migrator_object_version": 1,
    "migrator_version": 0,
    "name": "pypy37"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_object_version",
    "migrator_version",
    "name"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/570019929.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "OSXArm",
    "migrator_version": 1,
    "name": "arm osx addition"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "name"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/680501180.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "MigrationYaml",
    "migrator_object_version": 1,
    "migrator_version": 0,
    "name": "pypy37-windows"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_object_version",
    "migrator_version",
    "name"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/773875227.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "MigrationYaml",
    "migrator_object_version": 1,
    "migrator_version": 0,
    "name": "python310"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_object_version",
    "migrator_version",
    "name"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/846071835.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "ArchRebuild",
    "migrator_version": 1,
    "name": "aarch64 and ppc64le addition"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "name"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/904103506.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "MigrationYaml",
    "migrator_object_version": 1,
    "migrator_version": 0,
    "name": "pypy38"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_object_version",
    "migrator_version",
    "name"
   ]
  }
 ],
 "archived": false,
 "bad": false,
 "branch": "main",
 "conda-forge.yml": {
  "build_platform": {
   "osx_arm64": "osx_64"
  },
  "provider": {
   "linux_aarch64": "default",
   "linux_ppc64le": "default"
  }
 },
 "feedstock_name": "sentencepiece",
 "hash_type": "sha256",
 "linux_64_meta_yaml": {
  "about": {
   "description": "SentencePiece is an unsupervised text tokenizer and detokenizer mainly for\nNeural Network-based text generation systems where the vocabulary size is\npredetermined prior to the neural model training.\n\nSentencePiece implements subword units (e.g., byte-pair-encoding (BPE)\n[[Sennrich et al.](http://www.aclweb.org/anthology/P16-1162)]) and unigram\nlanguage model [[Kudo](https://arxiv.org/abs/1804.109590)]) with the\nextension of direct training from raw sentences. SentencePiece allows us to\nmake a purely end-to-end system that does not depend on language-specific\npre/postprocessing.\n",
   "home": "https://github.com/google/sentencepiece/",
   "license": "Apache-2.0",
   "license_family": "Apache",
   "license_file": "LICENSE",
   "summary": "Unsupervised text tokenizer for Neural Network-based text generation."
  },
  "build": {
   "number": "0"
  },
  "extra": {
   "recipe-maintainers": [
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari"
   ]
  },
  "package": {
   "name": "sentencepiece",
   "version": "0.1.96"
  },
  "requirements": {
   "build": [
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config"
   ],
   "host": [
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python"
   ],
   "run": [
    "python",
    "python",
    "python",
    "python",
    "python"
   ]
  },
  "source": {
   "sha256": "5198f31c3bb25e685e9e68355a3bf67a1db23c9e8bdccc33dc015f496a44df7a",
   "url": "https://github.com/google/sentencepiece/archive/v0.1.96.tar.gz"
  },
  "test": {
   "commands": [
    "pip check",
    "spm_export_vocab --help",
    "spm_normalize --help",
    "cd python && pytest test",
    "pip check",
    "spm_export_vocab --help",
    "spm_normalize --help",
    "cd python && pytest test",
    "pip check",
    "spm_export_vocab --help",
    "spm_normalize --help",
    "cd python && pytest test",
    "pip check",
    "spm_export_vocab --help",
    "spm_normalize --help",
    "cd python && pytest test",
    "pip check",
    "spm_export_vocab --help",
    "spm_normalize --help",
    "cd python && pytest test"
   ],
   "imports": [
    "sentencepiece",
    "sentencepiece",
    "sentencepiece",
    "sentencepiece",
    "sentencepiece"
   ],
   "requires": [
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest"
   ],
   "source_files": [
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data"
   ]
  }
 },
 "linux_64_requirements": {
  "build": {
   "__set__": true,
   "elements": [
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config"
   ]
  },
  "host": {
   "__set__": true,
   "elements": [
    "pip",
    "python"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "python"
   ]
  },
  "test": {
   "__set__": true,
   "elements": [
    "pip",
    "pytest"
   ]
  }
 },
 "linux_aarch64_meta_yaml": {
  "about": {
   "description": "SentencePiece is an unsupervised text tokenizer and detokenizer mainly for\nNeural Network-based text generation systems where the vocabulary size is\npredetermined prior to the neural model training.\n\nSentencePiece implements subword units (e.g., byte-pair-encoding (BPE)\n[[Sennrich et al.](http://www.aclweb.org/anthology/P16-1162)]) and unigram\nlanguage model [[Kudo](https://arxiv.org/abs/1804.109590)]) with the\nextension of direct training from raw sentences. SentencePiece allows us to\nmake a purely end-to-end system that does not depend on language-specific\npre/postprocessing.\n",
   "home": "https://github.com/google/sentencepiece/",
   "license": "Apache-2.0",
   "license_family": "Apache",
   "license_file": "LICENSE",
   "summary": "Unsupervised text tokenizer for Neural Network-based text generation."
  },
  "build": {
   "number": "0"
  },
  "extra": {
   "recipe-maintainers": [
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari"
   ]
  },
  "package": {
   "name": "sentencepiece",
   "version": "0.1.96"
  },
  "requirements": {
   "build": [
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config"
   ],
   "host": [
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python"
   ],
   "run": [
    "python",
    "python",
    "python",
    "python",
    "python"
   ]
  },
  "source": {
   "sha256": "5198f31c3bb25e685e9e68355a3bf67a1db23c9e8bdccc33dc015f496a44df7a",
   "url": "https://github.com/google/sentencepiece/archive/v0.1.96.tar.gz"
  },
  "test": {
   "commands": [
    "pip check",
    "spm_export_vocab --help",
    "spm_normalize --help",
    "cd python && pytest test",
    "pip check",
    "spm_export_vocab --help",
    "spm_normalize --help",
    "cd python && pytest test",
    "pip check",
    "spm_export_vocab --help",
    "spm_normalize --help",
    "cd python && pytest test",
    "pip check",
    "spm_export_vocab --help",
    "spm_normalize --help",
    "cd python && pytest test",
    "pip check",
    "spm_export_vocab --help",
    "spm_normalize --help",
    "cd python && pytest test"
   ],
   "imports": [
    "sentencepiece",
    "sentencepiece",
    "sentencepiece",
    "sentencepiece",
    "sentencepiece"
   ],
   "requires": [
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest"
   ],
   "source_files": [
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data"
   ]
  }
 },
 "linux_aarch64_requirements": {
  "build": {
   "__set__": true,
   "elements": [
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config"
   ]
  },
  "host": {
   "__set__": true,
   "elements": [
    "pip",
    "python"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "python"
   ]
  },
  "test": {
   "__set__": true,
   "elements": [
    "pip",
    "pytest"
   ]
  }
 },
 "linux_ppc64le_meta_yaml": {
  "about": {
   "description": "SentencePiece is an unsupervised text tokenizer and detokenizer mainly for\nNeural Network-based text generation systems where the vocabulary size is\npredetermined prior to the neural model training.\n\nSentencePiece implements subword units (e.g., byte-pair-encoding (BPE)\n[[Sennrich et al.](http://www.aclweb.org/anthology/P16-1162)]) and unigram\nlanguage model [[Kudo](https://arxiv.org/abs/1804.109590)]) with the\nextension of direct training from raw sentences. SentencePiece allows us to\nmake a purely end-to-end system that does not depend on language-specific\npre/postprocessing.\n",
   "home": "https://github.com/google/sentencepiece/",
   "license": "Apache-2.0",
   "license_family": "Apache",
   "license_file": "LICENSE",
   "summary": "Unsupervised text tokenizer for Neural Network-based text generation."
  },
  "build": {
   "number": "0"
  },
  "extra": {
   "recipe-maintainers": [
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari"
   ]
  },
  "package": {
   "name": "sentencepiece",
   "version": "0.1.96"
  },
  "requirements": {
   "build": [
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config"
   ],
   "host": [
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python"
   ],
   "run": [
    "python",
    "python",
    "python",
    "python",
    "python"
   ]
  },
  "source": {
   "sha256": "5198f31c3bb25e685e9e68355a3bf67a1db23c9e8bdccc33dc015f496a44df7a",
   "url": "https://github.com/google/sentencepiece/archive/v0.1.96.tar.gz"
  },
  "test": {
   "commands": [
    "pip check",
    "spm_export_vocab --help",
    "spm_normalize --help",
    "cd python && pytest test",
    "pip check",
    "spm_export_vocab --help",
    "spm_normalize --help",
    "cd python && pytest test",
    "pip check",
    "spm_export_vocab --help",
    "spm_normalize --help",
    "cd python && pytest test",
    "pip check",
    "spm_export_vocab --help",
    "spm_normalize --help",
    "cd python && pytest test",
    "pip check",
    "spm_export_vocab --help",
    "spm_normalize --help",
    "cd python && pytest test"
   ],
   "imports": [
    "sentencepiece",
    "sentencepiece",
    "sentencepiece",
    "sentencepiece",
    "sentencepiece"
   ],
   "requires": [
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest"
   ],
   "source_files": [
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data"
   ]
  }
 },
 "linux_ppc64le_requirements": {
  "build": {
   "__set__": true,
   "elements": [
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config"
   ]
  },
  "host": {
   "__set__": true,
   "elements": [
    "pip",
    "python"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "python"
   ]
  },
  "test": {
   "__set__": true,
   "elements": [
    "pip",
    "pytest"
   ]
  }
 },
 "meta_yaml": {
  "about": {
   "description": "SentencePiece is an unsupervised text tokenizer and detokenizer mainly for\nNeural Network-based text generation systems where the vocabulary size is\npredetermined prior to the neural model training.\n\nSentencePiece implements subword units (e.g., byte-pair-encoding (BPE)\n[[Sennrich et al.](http://www.aclweb.org/anthology/P16-1162)]) and unigram\nlanguage model [[Kudo](https://arxiv.org/abs/1804.109590)]) with the\nextension of direct training from raw sentences. SentencePiece allows us to\nmake a purely end-to-end system that does not depend on language-specific\npre/postprocessing.\n",
   "home": "https://github.com/google/sentencepiece/",
   "license": "Apache-2.0",
   "license_family": "Apache",
   "license_file": "LICENSE",
   "summary": "Unsupervised text tokenizer for Neural Network-based text generation."
  },
  "build": {
   "number": "0"
  },
  "extra": {
   "recipe-maintainers": [
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari"
   ]
  },
  "package": {
   "name": "sentencepiece",
   "version": "0.1.96"
  },
  "requirements": {
   "build": [
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "make",
    "pkg-config"
   ],
   "host": [
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python"
   ],
   "run": [
    "python",
    "python",
    "python",
    "python",
    "python",
    "python",
    "python",
    "python",
    "python",
    "python",
    "python",
    "python",
    "python",
    "python",
    "python",
    "python",
    "python",
    "python",
    "python",
    "python",
    "python",
    "python",
    "python",
    "python",
    "python",
    "python",
    "python",
    "python"
   ]
  },
  "source": {
   "sha256": "5198f31c3bb25e685e9e68355a3bf67a1db23c9e8bdccc33dc015f496a44df7a",
   "url": "https://github.com/google/sentencepiece/archive/v0.1.96.tar.gz"
  },
  "test": {
   "commands": [
    "pip check",
    "spm_export_vocab --help",
    "spm_normalize --help",
    "cd python && pytest test",
    "pip check",
    "spm_export_vocab --help",
    "spm_normalize --help",
    "cd python && pytest test",
    "pip check",
    "spm_export_vocab --help",
    "spm_normalize --help",
    "cd python && pytest test",
    "pip check",
    "spm_export_vocab --help",
    "spm_normalize --help",
    "cd python && pytest test",
    "pip check",
    "spm_export_vocab --help",
    "spm_normalize --help",
    "cd python && pytest test",
    "pip check",
    "spm_export_vocab --help",
    "spm_normalize --help",
    "cd python && pytest test",
    "pip check",
    "spm_export_vocab --help",
    "spm_normalize --help",
    "cd python && pytest test",
    "pip check",
    "spm_export_vocab --help",
    "spm_normalize --help",
    "cd python && pytest test",
    "pip check",
    "spm_export_vocab --help",
    "spm_normalize --help",
    "cd python && pytest test",
    "pip check",
    "spm_export_vocab --help",
    "spm_normalize --help",
    "cd python && pytest test",
    "pip check",
    "spm_export_vocab --help",
    "spm_normalize --help",
    "cd python && pytest test",
    "pip check",
    "spm_export_vocab --help",
    "spm_normalize --help",
    "cd python && pytest test",
    "pip check",
    "spm_export_vocab --help",
    "spm_normalize --help",
    "cd python && pytest test",
    "pip check",
    "spm_export_vocab --help",
    "spm_normalize --help",
    "cd python && pytest test",
    "pip check",
    "spm_export_vocab --help",
    "spm_normalize --help",
    "cd python && pytest test",
    "pip check",
    "cd python && pytest test",
    "pip check",
    "cd python && pytest test",
    "pip check",
    "cd python && pytest test",
    "pip check",
    "cd python && pytest test",
    "pip check",
    "cd python && pytest test",
    "pip check",
    "cd python && pytest test",
    "pip check",
    "cd python && pytest test",
    "pip check",
    "cd python && pytest test",
    "pip check",
    "cd python && pytest test",
    "pip check",
    "cd python && pytest test",
    "pip check",
    "cd python && pytest test",
    "pip check",
    "cd python && pytest test",
    "pip check",
    "cd python && pytest test"
   ],
   "imports": [
    "sentencepiece",
    "sentencepiece",
    "sentencepiece",
    "sentencepiece",
    "sentencepiece",
    "sentencepiece",
    "sentencepiece",
    "sentencepiece",
    "sentencepiece",
    "sentencepiece",
    "sentencepiece",
    "sentencepiece",
    "sentencepiece",
    "sentencepiece",
    "sentencepiece",
    "sentencepiece",
    "sentencepiece",
    "sentencepiece",
    "sentencepiece",
    "sentencepiece",
    "sentencepiece",
    "sentencepiece",
    "sentencepiece",
    "sentencepiece",
    "sentencepiece",
    "sentencepiece",
    "sentencepiece",
    "sentencepiece"
   ],
   "requires": [
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest"
   ],
   "source_files": [
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data"
   ]
  }
 },
 "name": "sentencepiece",
 "new_version": "1.0.0",
 "new_version_attempts": {
  "0.1.92": 1,
  "1.0.0": 1
 },
 "new_version_errors": {},
 "osx_64_meta_yaml": {
  "about": {
   "description": "SentencePiece is an unsupervised text tokenizer and detokenizer mainly for\nNeural Network-based text generation systems where the vocabulary size is\npredetermined prior to the neural model training.\n\nSentencePiece implements subword units (e.g., byte-pair-encoding (BPE)\n[[Sennrich et al.](http://www.aclweb.org/anthology/P16-1162)]) and unigram\nlanguage model [[Kudo](https://arxiv.org/abs/1804.109590)]) with the\nextension of direct training from raw sentences. SentencePiece allows us to\nmake a purely end-to-end system that does not depend on language-specific\npre/postprocessing.\n",
   "home": "https://github.com/google/sentencepiece/",
   "license": "Apache-2.0",
   "license_family": "Apache",
   "license_file": "LICENSE",
   "summary": "Unsupervised text tokenizer for Neural Network-based text generation."
  },
  "build": {
   "number": "0"
  },
  "extra": {
   "recipe-maintainers": [
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari"
   ]
  },
  "package": {
   "name": "sentencepiece",
   "version": "0.1.96"
  },
  "requirements": {
   "build": [
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config"
   ],
   "host": [
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python"
   ],
   "run": [
    "python",
    "python",
    "python",
    "python",
    "python"
   ]
  },
  "source": {
   "sha256": "5198f31c3bb25e685e9e68355a3bf67a1db23c9e8bdccc33dc015f496a44df7a",
   "url": "https://github.com/google/sentencepiece/archive/v0.1.96.tar.gz"
  },
  "test": {
   "commands": [
    "pip check",
    "cd python && pytest test",
    "pip check",
    "cd python && pytest test",
    "pip check",
    "cd python && pytest test",
    "pip check",
    "cd python && pytest test",
    "pip check",
    "cd python && pytest test"
   ],
   "imports": [
    "sentencepiece",
    "sentencepiece",
    "sentencepiece",
    "sentencepiece",
    "sentencepiece"
   ],
   "requires": [
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest"
   ],
   "source_files": [
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data"
   ]
  }
 },
 "osx_64_requirements": {
  "build": {
   "__set__": true,
   "elements": [
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config"
   ]
  },
  "host": {
   "__set__": true,
   "elements": [
    "pip",
    "python"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "python"
   ]
  },
  "test": {
   "__set__": true,
   "elements": [
    "pip",
    "pytest"
   ]
  }
 },
 "osx_arm64_meta_yaml": {
  "about": {
   "description": "SentencePiece is an unsupervised text tokenizer and detokenizer mainly for\nNeural Network-based text generation systems where the vocabulary size is\npredetermined prior to the neural model training.\n\nSentencePiece implements subword units (e.g., byte-pair-encoding (BPE)\n[[Sennrich et al.](http://www.aclweb.org/anthology/P16-1162)]) and unigram\nlanguage model [[Kudo](https://arxiv.org/abs/1804.109590)]) with the\nextension of direct training from raw sentences. SentencePiece allows us to\nmake a purely end-to-end system that does not depend on language-specific\npre/postprocessing.\n",
   "home": "https://github.com/google/sentencepiece/",
   "license": "Apache-2.0",
   "license_family": "Apache",
   "license_file": "LICENSE",
   "summary": "Unsupervised text tokenizer for Neural Network-based text generation."
  },
  "build": {
   "number": "0"
  },
  "extra": {
   "recipe-maintainers": [
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari"
   ]
  },
  "package": {
   "name": "sentencepiece",
   "version": "0.1.96"
  },
  "requirements": {
   "build": [
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config"
   ],
   "host": [
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python"
   ],
   "run": [
    "python",
    "python",
    "python"
   ]
  },
  "source": {
   "sha256": "5198f31c3bb25e685e9e68355a3bf67a1db23c9e8bdccc33dc015f496a44df7a",
   "url": "https://github.com/google/sentencepiece/archive/v0.1.96.tar.gz"
  },
  "test": {
   "commands": [
    "pip check",
    "cd python && pytest test",
    "pip check",
    "cd python && pytest test",
    "pip check",
    "cd python && pytest test"
   ],
   "imports": [
    "sentencepiece",
    "sentencepiece",
    "sentencepiece"
   ],
   "requires": [
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest"
   ],
   "source_files": [
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data"
   ]
  }
 },
 "osx_arm64_requirements": {
  "build": {
   "__set__": true,
   "elements": [
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config"
   ]
  },
  "host": {
   "__set__": true,
   "elements": [
    "pip",
    "python"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "python"
   ]
  },
  "test": {
   "__set__": true,
   "elements": [
    "pip",
    "pytest"
   ]
  }
 },
 "outputs_names": {
  "__set__": true,
  "elements": [
   "sentencepiece"
  ]
 },
 "pinning_version": "2022.04.08.13.19.51",
 "raw_meta_yaml": "{% set version = \"0.1.96\" %}\n\npackage:\n  name: sentencepiece\n  version: {{ version }}\n\nsource:\n  url: https://github.com/google/sentencepiece/archive/v{{ version }}.tar.gz\n  sha256: 5198f31c3bb25e685e9e68355a3bf67a1db23c9e8bdccc33dc015f496a44df7a\n\nbuild:\n  number: 0\n\nrequirements:\n  build:\n    - python                                 # [build_platform != target_platform]\n    - cross-python_{{ target_platform }}     # [build_platform != target_platform]\n    - cmake\n    - {{ compiler('cxx') }}\n    - gperftools  # [unix]\n    - make\n    - pkg-config\n  host:\n    - pip\n    - python\n  run:\n    - python\n\ntest:\n  imports:\n    - sentencepiece\n  requires:\n    - pip\n    - pytest\n  source_files:\n    - python/test\n    - data\n  commands:\n    - pip check\n    - spm_export_vocab --help  # [linux]\n    - spm_normalize --help     # [linux]\n    # upstream test suite expects to be run from PKG_ROOT/python\n    - cd python && pytest test\n\nabout:\n  home: \"https://github.com/google/sentencepiece/\"\n  license: Apache-2.0\n  license_family: Apache\n  license_file: LICENSE\n  summary: Unsupervised text tokenizer for Neural Network-based text generation.\n  description: |\n    SentencePiece is an unsupervised text tokenizer and detokenizer mainly for\n    Neural Network-based text generation systems where the vocabulary size is\n    predetermined prior to the neural model training.\n\n    SentencePiece implements subword units (e.g., byte-pair-encoding (BPE)\n    [[Sennrich et al.](http://www.aclweb.org/anthology/P16-1162)]) and unigram\n    language model [[Kudo](https://arxiv.org/abs/1804.109590)]) with the\n    extension of direct training from raw sentences. SentencePiece allows us to\n    make a purely end-to-end system that does not depend on language-specific\n    pre/postprocessing.\n\nextra:\n  recipe-maintainers:\n    - setu4993\n    - rluria14\n    - ndmaxar\n    - oblute\n    - h-vetinari\n",
 "req": {
  "__set__": true,
  "elements": [
   "cmake",
   "cxx_compiler_stub",
   "gperftools",
   "make",
   "pip",
   "pkg-config",
   "python"
  ]
 },
 "requirements": {
  "build": {
   "__set__": true,
   "elements": [
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config"
   ]
  },
  "host": {
   "__set__": true,
   "elements": [
    "cxx_compiler_stub",
    "pip",
    "python"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "cxx_compiler_stub",
    "python"
   ]
  },
  "test": {
   "__set__": true,
   "elements": [
    "pip",
    "pytest"
   ]
  }
 },
 "smithy_version": "3.19.0",
 "strong_exports": false,
 "total_requirements": {
  "build": {
   "__set__": true,
   "elements": [
    "cmake",
    "cxx_compiler_stub",
    "gperftools",
    "make",
    "pkg-config"
   ]
  },
  "host": {
   "__set__": true,
   "elements": [
    "pip",
    "python"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "python"
   ]
  },
  "test": {
   "__set__": true,
   "elements": [
    "pip",
    "pytest"
   ]
  }
 },
 "url": "https://github.com/google/sentencepiece/archive/v0.1.96.tar.gz",
 "version": "0.1.96",
 "win_64_meta_yaml": {
  "about": {
   "description": "SentencePiece is an unsupervised text tokenizer and detokenizer mainly for\nNeural Network-based text generation systems where the vocabulary size is\npredetermined prior to the neural model training.\n\nSentencePiece implements subword units (e.g., byte-pair-encoding (BPE)\n[[Sennrich et al.](http://www.aclweb.org/anthology/P16-1162)]) and unigram\nlanguage model [[Kudo](https://arxiv.org/abs/1804.109590)]) with the\nextension of direct training from raw sentences. SentencePiece allows us to\nmake a purely end-to-end system that does not depend on language-specific\npre/postprocessing.\n",
   "home": "https://github.com/google/sentencepiece/",
   "license": "Apache-2.0",
   "license_family": "Apache",
   "license_file": "LICENSE",
   "summary": "Unsupervised text tokenizer for Neural Network-based text generation."
  },
  "build": {
   "number": "0"
  },
  "extra": {
   "recipe-maintainers": [
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari",
    "setu4993",
    "rluria14",
    "ndmaxar",
    "oblute",
    "h-vetinari"
   ]
  },
  "package": {
   "name": "sentencepiece",
   "version": "0.1.96"
  },
  "requirements": {
   "build": [
    "cmake",
    "cxx_compiler_stub",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "make",
    "pkg-config",
    "cmake",
    "cxx_compiler_stub",
    "make",
    "pkg-config"
   ],
   "host": [
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python",
    "pip",
    "python"
   ],
   "run": [
    "python",
    "python",
    "python",
    "python",
    "python"
   ]
  },
  "source": {
   "sha256": "5198f31c3bb25e685e9e68355a3bf67a1db23c9e8bdccc33dc015f496a44df7a",
   "url": "https://github.com/google/sentencepiece/archive/v0.1.96.tar.gz"
  },
  "test": {
   "commands": [
    "pip check",
    "cd python && pytest test",
    "pip check",
    "cd python && pytest test",
    "pip check",
    "cd python && pytest test",
    "pip check",
    "cd python && pytest test",
    "pip check",
    "cd python && pytest test"
   ],
   "imports": [
    "sentencepiece",
    "sentencepiece",
    "sentencepiece",
    "sentencepiece",
    "sentencepiece"
   ],
   "requires": [
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest",
    "pip",
    "pytest"
   ],
   "source_files": [
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data",
    "python/test",
    "data"
   ]
  }
 },
 "win_64_requirements": {
  "build": {
   "__set__": true,
   "elements": [
    "cmake",
    "cxx_compiler_stub",
    "make",
    "pkg-config"
   ]
  },
  "host": {
   "__set__": true,
   "elements": [
    "pip",
    "python"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "python"
   ]
  },
  "test": {
   "__set__": true,
   "elements": [
    "pip",
    "pytest"
   ]
  }
 }
}