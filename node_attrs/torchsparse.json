{
 "PRed": [
  {
   "PR": {
    "__lazy_json__": "pr_json/663057630.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "MigrationYaml",
    "migrator_object_version": 1,
    "migrator_version": 0,
    "name": "cuda110"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_object_version",
    "migrator_version",
    "name"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/fe042d4d-5ac7-482d-9644-1f2c9f62797e.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "MigrationYaml",
    "migrator_object_version": 1,
    "migrator_version": 0,
    "name": "cuda111_112"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_object_version",
    "migrator_version",
    "name"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/663100535.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "OSXArm",
    "migrator_version": 1,
    "name": "arm osx addition"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "name"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/677577826.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "Version",
    "migrator_version": 0,
    "version": "1.4.0"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "version"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/833081151.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "MigrationYaml",
    "migrator_object_version": 1,
    "migrator_version": 0,
    "name": "pytorch110"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_object_version",
    "migrator_version",
    "name"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/884745992.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "MigrationYaml",
    "migrator_object_version": 1,
    "migrator_version": 0,
    "name": "pytorch111"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_object_version",
    "migrator_version",
    "name"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/884888162.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "MigrationYaml",
    "migrator_object_version": 1,
    "migrator_version": 0,
    "name": "python310"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_object_version",
    "migrator_version",
    "name"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/1000628555.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "MigrationYaml",
    "migrator_object_version": 1,
    "migrator_version": 0,
    "name": "pytorch112"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_object_version",
    "migrator_version",
    "name"
   ]
  }
 ],
 "archived": false,
 "bad": "make_graph: render error No module named 'toml'\nTraceback (most recent call last):\n  File \"/home/runner/work/autotick-bot/autotick-bot/cf-scripts/conda_forge_tick/feedstock_parser.py\", line 241, in populate_feedstock_attributes\n    parse_meta_yaml(\n  File \"/home/runner/work/autotick-bot/autotick-bot/cf-scripts/conda_forge_tick/utils.py\", line 167, in parse_meta_yaml\n    return _parse_meta_yaml_impl(\n  File \"/home/runner/work/autotick-bot/autotick-bot/cf-scripts/conda_forge_tick/utils.py\", line 239, in _parse_meta_yaml_impl\n    m = MetaData(tmpdir, config=config, variant=var)\n  File \"/usr/share/miniconda3/envs/run_env/lib/python3.9/site-packages/conda_build/metadata.py\", line 932, in __init__\n    self.parse_again(permit_undefined_jinja=True, allow_no_other_outputs=True)\n  File \"/usr/share/miniconda3/envs/run_env/lib/python3.9/site-packages/conda_build/metadata.py\", line 1007, in parse_again\n    self.meta = parse(self._get_contents(permit_undefined_jinja,\n  File \"/usr/share/miniconda3/envs/run_env/lib/python3.9/site-packages/conda_build/metadata.py\", line 1546, in _get_contents\n    from conda_build.jinja_context import context_processor, UndefinedNeverFail, FilteredLoader\n  File \"/usr/share/miniconda3/envs/run_env/lib/python3.9/site-packages/conda_build/jinja_context.py\", line 13, in <module>\n    import toml\nModuleNotFoundError: No module named 'toml'\n",
 "branch": "main",
 "conda-forge.yml": {
  "build_platform": {
   "osx_arm64": "osx_64"
  }
 },
 "feedstock_name": "torchsparse",
 "hash_type": "sha256",
 "name": "torchsparse",
 "new_version": "1.4.0",
 "new_version_attempts": {
  "1.4.0": 1
 },
 "new_version_errors": {},
 "outputs_names": {
  "__set__": true,
  "elements": [
   "torchsparse"
  ]
 },
 "pinning_version": "2022.07.18.11.18.34",
 "pre_pr_migrator_attempts": {},
 "pre_pr_migrator_status": {},
 "raw_meta_yaml": "{% set name = \"torchsparse\" %}\n{% set version = \"1.4.0\" %}\n\n# see github.com/conda-forge/conda-forge.github.io/issues/1059 for naming discussion\n{% set torch_proc_type = \"cuda\" if cuda_compiler_version != \"None\" else \"cpu\" %}\n\npackage:\n  name: {{ name|lower }}\n  version: {{ version }}\n\nsource:\n  url: https://github.com/mit-han-lab/{{ name }}/archive/v{{ version }}.tar.gz\n  sha256: a1cf3e8ecabb0f81cab7e14ed5ed0a31c8cb62b73d8ae172c50a779a82aae640\n\nbuild:\n  number: 14\n  skip: true  # [win]\n  string: py{{ CONDA_PY }}h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}_{{ torch_proc_type }}  # [cuda_compiler_version == \"None\"]\n  string: py{{ CONDA_PY }}cuda{{ cuda_compiler_version|replace('.', '') }}h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}_{{ torch_proc_type }}  # [cuda_compiler_version != \"None\"]\n  run_exports:\n    # ensure matching proc-type\n    - torchvision =*=*_{{ torch_proc_type }}\n\nrequirements:\n  build:\n    - python  # [build_platform != target_platform]\n    - cross-python_{{ target_platform }}  # [build_platform != target_platform]\n    - {{ compiler('c') }}\n    - {{ compiler('cxx') }}\n    - {{ compiler('cuda') }}  # [linux64 and cuda_compiler_version != 'None']\n    - sysroot_linux-64 ==2.17  # [linux64]\n    - pytorch ={{ pytorch }}={{ torch_proc_type }}*  # [build_platform != target_platform]\n  host:\n    - python\n    - pip\n    - sparsehash\n    # Leaving two dependencies helps rerender correctly\n    # The first gets filled in by the global pinnings\n    # The second gets the processor type\n    - pytorch\n    - pytorch =*={{ torch_proc_type }}*\n  run:\n    - python\n  run_constrained:\n    # 2022/02/05 hmaarrfk\n    # While conda packaging seems to allow us to specify\n    # constraints on the same package in different lines\n    # the resulting package doesn't have the ability to\n    # be specified in multiples lines\n    # This makes it tricky to use run_exports\n    # we add the GPU constraint in the run_constrained\n    # to allow us to have \"two\" constraints on the\n    # running package\n    - pytorch =*={{ torch_proc_type }}*\n\ntest:\n  imports:\n    - {{ name }}\n  requires:\n    - pip\n  commands:\n    - pip check\n\nabout:\n  home: https://github.com/mit-han-lab/torchsparse\n  license: MIT\n  license_family: MIT\n  license_file: LICENSE\n  summary: A high-performance computing library for efficient 3D sparse convolution.\n  description: |\n    A high-performance computing library for efficient 3D sparse convolution.\n  dev_url: https://github.com/mit-han-lab/{{ name }}\n\nextra:\n  recipe-maintainers:\n    - benjaminrwilson\n",
 "smithy_version": "3.21.0",
 "strong_exports": false,
 "url": "https://github.com/mit-han-lab/torchsparse/archive/v1.4.0.tar.gz",
 "version": "1.4.0"
}