{
 "PRed": [
  {
   "PR": {
    "__lazy_json__": "pr_json/464856127.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "Version",
    "migrator_version": 0,
    "version": "1.3.2"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "version"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/471866466.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "Version",
    "migrator_version": 0,
    "version": "1.3.3"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "version"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/490009778.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "Version",
    "migrator_version": 0,
    "version": "1.4.0"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "version"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/497147344.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "Version",
    "migrator_version": 0,
    "version": "1.4.1"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "version"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/519094294.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "Version",
    "migrator_version": 0,
    "version": "1.4.2"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "version"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/525044675.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "Version",
    "migrator_version": 0,
    "version": "1.4.3"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "version"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/529933638.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "Version",
    "migrator_version": 0,
    "version": "1.5.0"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "version"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/552052174.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "Version",
    "migrator_version": 0,
    "version": "1.5.1"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "version"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/576852353.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "Version",
    "migrator_version": 0,
    "version": "1.5.2"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "version"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/592443451.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "Version",
    "migrator_version": 0,
    "version": "1.5.3"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "version"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/594045351.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "Version",
    "migrator_version": 0,
    "version": "1.6.0"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "version"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/617208339.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "Version",
    "migrator_version": 0,
    "version": "1.6.1"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "version"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/649580174.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "Version",
    "migrator_version": 0,
    "version": "1.6.2"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "version"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/670786418.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "Version",
    "migrator_version": 0,
    "version": "1.7.0"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "version"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/687828998.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "Version",
    "migrator_version": 0,
    "version": "1.7.1"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "version"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/722941718.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "Version",
    "migrator_version": 0,
    "version": "1.7.2"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "version"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/742787502.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "Version",
    "migrator_version": 0,
    "version": "1.8.0"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "version"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/759362357.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "Version",
    "migrator_version": 0,
    "version": "1.8.1"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "version"
   ]
  }
 ],
 "archived": false,
 "bad": false,
 "branch": "main",
 "conda-forge.yml": {},
 "feedstock_name": "adversarial-robustness-toolbox",
 "hash_type": "sha256",
 "linux_64_meta_yaml": {
  "about": {
   "dev_url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox",
   "doc_url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/wiki/Documentation",
   "home": "https://github.com/Trusted-AI/adversarial-robustness-toolbox",
   "license": "MIT",
   "license_family": "MIT",
   "license_file": "LICENSE",
   "summary": "Toolbox for adversarial machine learning."
  },
  "build": {
   "noarch": "python",
   "number": "0",
   "script": "/usr/share/miniconda3/envs/run_env/conda-bld/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pla/bin/python -m pip install . -vv"
  },
  "extra": {
   "recipe-maintainers": [
    "mxr-conda",
    "rluria14",
    "oblute"
   ]
  },
  "package": {
   "name": "adversarial-robustness-toolbox",
   "version": "1.7.2"
  },
  "requirements": {
   "host": [
    "pip",
    "python >=3.6"
   ],
   "run": [
    "cma",
    "ffmpeg-python",
    "matplotlib-base",
    "mypy",
    "pillow",
    "pydub",
    "kornia",
    "python >=3.6",
    "resampy",
    "scikit-learn ==0.22.1",
    "scipy ==1.4.1",
    "six",
    "statsmodels",
    "tqdm",
    "tensorflow"
   ]
  },
  "source": {
   "sha256": "d5792c95c2e033bc6323c9b92f45f8b2b8f1e979bfbc697a97253b5b908e3280",
   "url": "https://pypi.io/packages/source/a/adversarial-robustness-toolbox/adversarial-robustness-toolbox-1.7.2.tar.gz"
  },
  "test": {
   "imports": [
    "art",
    "art.attacks",
    "art.attacks.evasion",
    "art.attacks.evasion.adversarial_patch",
    "art.attacks.evasion.projected_gradient_descent",
    "art.attacks.extraction",
    "art.attacks.inference",
    "art.attacks.poisoning",
    "art.attacks.poisoning.perturbations",
    "art.classifiers",
    "art.classifiers.scikitlearn",
    "art.defences",
    "art.defences.detector",
    "art.defences.detector.evasion",
    "art.defences.detector.evasion.subsetscanning",
    "art.defences.detector.poison",
    "art.defences.postprocessor",
    "art.defences.preprocessor",
    "art.defences.trainer",
    "art.defences.transformer",
    "art.estimators",
    "art.estimators.certification",
    "art.estimators.certification.randomized_smoothing",
    "art.estimators.classification",
    "art.estimators.encoding",
    "art.estimators.generation",
    "art.estimators.object_detection",
    "art.estimators.regression",
    "art.metrics",
    "art.wrappers",
    "tests",
    "tests.attacks",
    "tests.attacks.evasion",
    "tests.attacks.inference",
    "tests.classifiersFrameworks",
    "tests.defences",
    "tests.defences.detector",
    "tests.defences.detector.evasion",
    "tests.defences.detector.evasion.subsetscanning",
    "tests.defences.detector.poison",
    "tests.estimators",
    "tests.estimators.certification",
    "tests.estimators.classification",
    "tests.metrics",
    "tests.wrappers"
   ]
  }
 },
 "linux_64_requirements": {
  "build": {
   "__set__": true,
   "elements": []
  },
  "host": {
   "__set__": true,
   "elements": [
    "pip",
    "python"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "cma",
    "ffmpeg-python",
    "kornia",
    "matplotlib-base",
    "mypy",
    "pillow",
    "pydub",
    "python",
    "resampy",
    "scikit-learn",
    "scipy",
    "six",
    "statsmodels",
    "tensorflow",
    "tqdm"
   ]
  },
  "test": {
   "__set__": true,
   "elements": []
  }
 },
 "meta_yaml": {
  "about": {
   "dev_url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox",
   "doc_url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/wiki/Documentation",
   "home": "https://github.com/Trusted-AI/adversarial-robustness-toolbox",
   "license": "MIT",
   "license_family": "MIT",
   "license_file": "LICENSE",
   "summary": "Toolbox for adversarial machine learning."
  },
  "build": {
   "noarch": "python",
   "number": "0",
   "script": "/usr/share/miniconda3/envs/run_env/conda-bld/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pla/bin/python -m pip install . -vv"
  },
  "extra": {
   "recipe-maintainers": [
    "mxr-conda",
    "rluria14",
    "oblute"
   ]
  },
  "package": {
   "name": "adversarial-robustness-toolbox",
   "version": "1.7.2"
  },
  "requirements": {
   "host": [
    "pip",
    "python >=3.6"
   ],
   "run": [
    "cma",
    "ffmpeg-python",
    "matplotlib-base",
    "mypy",
    "pillow",
    "pydub",
    "kornia",
    "python >=3.6",
    "resampy",
    "scikit-learn ==0.22.1",
    "scipy ==1.4.1",
    "six",
    "statsmodels",
    "tqdm",
    "tensorflow"
   ]
  },
  "source": {
   "sha256": "d5792c95c2e033bc6323c9b92f45f8b2b8f1e979bfbc697a97253b5b908e3280",
   "url": "https://pypi.io/packages/source/a/adversarial-robustness-toolbox/adversarial-robustness-toolbox-1.7.2.tar.gz"
  },
  "test": {
   "imports": [
    "art",
    "art.attacks",
    "art.attacks.evasion",
    "art.attacks.evasion.adversarial_patch",
    "art.attacks.evasion.projected_gradient_descent",
    "art.attacks.extraction",
    "art.attacks.inference",
    "art.attacks.poisoning",
    "art.attacks.poisoning.perturbations",
    "art.classifiers",
    "art.classifiers.scikitlearn",
    "art.defences",
    "art.defences.detector",
    "art.defences.detector.evasion",
    "art.defences.detector.evasion.subsetscanning",
    "art.defences.detector.poison",
    "art.defences.postprocessor",
    "art.defences.preprocessor",
    "art.defences.trainer",
    "art.defences.transformer",
    "art.estimators",
    "art.estimators.certification",
    "art.estimators.certification.randomized_smoothing",
    "art.estimators.classification",
    "art.estimators.encoding",
    "art.estimators.generation",
    "art.estimators.object_detection",
    "art.estimators.regression",
    "art.metrics",
    "art.wrappers",
    "tests",
    "tests.attacks",
    "tests.attacks.evasion",
    "tests.attacks.inference",
    "tests.classifiersFrameworks",
    "tests.defences",
    "tests.defences.detector",
    "tests.defences.detector.evasion",
    "tests.defences.detector.evasion.subsetscanning",
    "tests.defences.detector.poison",
    "tests.estimators",
    "tests.estimators.certification",
    "tests.estimators.classification",
    "tests.metrics",
    "tests.wrappers"
   ]
  }
 },
 "name": "adversarial-robustness-toolbox",
 "new_version": "1.9.0",
 "new_version_attempts": {
  "1.3.2": 1,
  "1.3.3": 1,
  "1.4.0": 1,
  "1.4.1": 1,
  "1.4.2": 1,
  "1.4.3": 1,
  "1.5.0": 1,
  "1.5.1": 1,
  "1.5.2": 1,
  "1.5.3": 1,
  "1.6.0": 1,
  "1.6.1": 1,
  "1.6.2": 1,
  "1.7.0": 1,
  "1.7.1": 1,
  "1.7.2": 1,
  "1.8.0": 1,
  "1.8.1": 1,
  "1.9.0": 1
 },
 "new_version_errors": {
  "1.9.0": "The recipe did not change in the version migration, a URL did not hash, or there is jinja2 syntax the bot cannot handle!\n\nPlease check the URLs in your recipe with version '1.9.0' to make sure they exist!\n\nWe also found the following errors:\n\n - could not hash URL template 'https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/{{ name }}-{{ version }}.tar.gz'\n"
 },
 "outputs_names": {
  "__set__": true,
  "elements": [
   "adversarial-robustness-toolbox"
  ]
 },
 "pinning_version": "2021.10.13.20.28.53",
 "raw_meta_yaml": "{% set name = \"adversarial-robustness-toolbox\" %}\n{% set version = \"1.7.2\" %}\n\npackage:\n  name: {{ name|lower }}\n  version: {{ version }}\n\nsource:\n  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/{{ name }}-{{ version }}.tar.gz\n  sha256: d5792c95c2e033bc6323c9b92f45f8b2b8f1e979bfbc697a97253b5b908e3280\n\nbuild:\n  number: 0\n  noarch: python\n  script: {{ PYTHON }} -m pip install . -vv\n\nrequirements:\n  host:\n    - pip\n    - python >=3.6\n  run:\n    - cma\n    - ffmpeg-python\n    - matplotlib-base\n    - mypy\n    - pillow\n    - pydub\n    - kornia\n    - python >=3.6\n    - resampy\n    - scikit-learn ==0.22.1\n    - scipy ==1.4.1\n    - six\n    - statsmodels\n    - tqdm\n    - tensorflow\n\ntest:\n  imports:\n    - art\n    - art.attacks\n    - art.attacks.evasion\n    - art.attacks.evasion.adversarial_patch\n    - art.attacks.evasion.projected_gradient_descent\n    - art.attacks.extraction\n    - art.attacks.inference\n    - art.attacks.poisoning\n    - art.attacks.poisoning.perturbations\n    - art.classifiers\n    - art.classifiers.scikitlearn\n    - art.defences\n    - art.defences.detector\n    - art.defences.detector.evasion\n    - art.defences.detector.evasion.subsetscanning\n    - art.defences.detector.poison\n    - art.defences.postprocessor\n    - art.defences.preprocessor\n    - art.defences.trainer\n    - art.defences.transformer\n    - art.estimators\n    - art.estimators.certification\n    - art.estimators.certification.randomized_smoothing\n    - art.estimators.classification\n    - art.estimators.encoding\n    - art.estimators.generation\n    - art.estimators.object_detection\n    - art.estimators.regression\n    - art.metrics\n    - art.wrappers\n    - tests\n    - tests.attacks\n    - tests.attacks.evasion\n    - tests.attacks.inference\n    - tests.classifiersFrameworks\n    - tests.defences\n    - tests.defences.detector\n    - tests.defences.detector.evasion\n    - tests.defences.detector.evasion.subsetscanning\n    - tests.defences.detector.poison\n    - tests.estimators\n    - tests.estimators.certification\n    - tests.estimators.classification\n    - tests.metrics\n    - tests.wrappers\n\nabout:\n  home: https://github.com/Trusted-AI/adversarial-robustness-toolbox\n  license: MIT\n  license_family: MIT\n  license_file: LICENSE\n  summary: Toolbox for adversarial machine learning.\n  doc_url: https://github.com/Trusted-AI/adversarial-robustness-toolbox/wiki/Documentation\n  dev_url: https://github.com/Trusted-AI/adversarial-robustness-toolbox\n\nextra:\n  recipe-maintainers:\n    - mxr-conda\n    - rluria14\n    - oblute\n",
 "req": {
  "__set__": true,
  "elements": [
   "cma",
   "ffmpeg-python",
   "kornia",
   "matplotlib-base",
   "mypy",
   "pillow",
   "pip",
   "pydub",
   "python",
   "resampy",
   "scikit-learn",
   "scipy",
   "six",
   "statsmodels",
   "tensorflow",
   "tqdm"
  ]
 },
 "requirements": {
  "build": {
   "__set__": true,
   "elements": []
  },
  "host": {
   "__set__": true,
   "elements": [
    "pip",
    "python"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "cma",
    "ffmpeg-python",
    "kornia",
    "matplotlib-base",
    "mypy",
    "pillow",
    "pydub",
    "python",
    "resampy",
    "scikit-learn",
    "scipy",
    "six",
    "statsmodels",
    "tensorflow",
    "tqdm"
   ]
  },
  "test": {
   "__set__": true,
   "elements": []
  }
 },
 "smithy_version": "3.12",
 "strong_exports": false,
 "total_requirements": {
  "build": {
   "__set__": true,
   "elements": []
  },
  "host": {
   "__set__": true,
   "elements": [
    "pip",
    "python >=3.6"
   ]
  },
  "run": {
   "__set__": true,
   "elements": [
    "cma",
    "ffmpeg-python",
    "kornia",
    "matplotlib-base",
    "mypy",
    "pillow",
    "pydub",
    "python >=3.6",
    "resampy",
    "scikit-learn ==0.22.1",
    "scipy ==1.4.1",
    "six",
    "statsmodels",
    "tensorflow",
    "tqdm"
   ]
  },
  "test": {
   "__set__": true,
   "elements": []
  }
 },
 "url": "https://pypi.io/packages/source/a/adversarial-robustness-toolbox/adversarial-robustness-toolbox-1.7.2.tar.gz",
 "version": "1.7.2"
}