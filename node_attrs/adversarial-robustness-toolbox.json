{
 "PRed":[
  {
   "PR":{
    "__lazy_json__":"pr_json/464856127.json"
   },
   "data":{
    "bot_rerun":false,
    "migrator_name":"Version",
    "migrator_version":0,
    "version":"1.3.2"
   },
   "keys":[
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "version"
   ]
  },
  {
   "PR":{
    "__lazy_json__":"pr_json/471866466.json"
   },
   "data":{
    "bot_rerun":false,
    "migrator_name":"Version",
    "migrator_version":0,
    "version":"1.3.3"
   },
   "keys":[
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "version"
   ]
  }
 ],
 "archived":false,
 "bad":false,
 "conda-forge.yml":{},
 "feedstock_name":"adversarial-robustness-toolbox",
 "hash_type":"sha256",
 "linux_64_meta_yaml":{
  "about":{
   "dev_url":"https://github.com/Trusted-AI/adversarial-robustness-toolbox",
   "doc_url":"https://github.com/Trusted-AI/adversarial-robustness-toolbox/wiki/Documentation",
   "home":"https://github.com/Trusted-AI/adversarial-robustness-toolbox",
   "license":"MIT",
   "license_family":"MIT",
   "license_file":"LICENSE",
   "summary":"Toolbox for adversarial machine learning."
  },
  "build":{
   "noarch":"python",
   "number":"0",
   "script":"-m pip install . -vv"
  },
  "extra":{
   "recipe-maintainers":[
    "rluria14",
    "oblute",
    "ndmaxar"
   ]
  },
  "package":{
   "name":"adversarial-robustness-toolbox",
   "version":"1.3.3"
  },
  "requirements":{
   "host":[
    "pip",
    "python"
   ],
   "run":[
    "cma",
    "ffmpeg-python",
    "matplotlib-base",
    "mypy",
    "pillow",
    "pydub",
    "python",
    "resampy",
    "scikit-learn ==0.22.1",
    "scipy ==1.4.1",
    "six",
    "statsmodels",
    "tqdm"
   ]
  },
  "source":{
   "sha256":"204e9db634e39fb05ab72fbfbd4762ec71ac085b22264c2718e376be111b08d1",
   "url":"https://pypi.io/packages/source/a/adversarial-robustness-toolbox/adversarial-robustness-toolbox-1.3.3.tar.gz"
  },
  "test":{
   "imports":[
    "art",
    "art.attacks",
    "art.attacks.evasion",
    "art.attacks.evasion.adversarial_patch",
    "art.attacks.evasion.projected_gradient_descent",
    "art.attacks.extraction",
    "art.attacks.inference",
    "art.attacks.poisoning",
    "art.attacks.poisoning.perturbations",
    "art.classifiers",
    "art.classifiers.scikitlearn",
    "art.defences",
    "art.defences.detector",
    "art.defences.detector.evasion",
    "art.defences.detector.evasion.subsetscanning",
    "art.defences.detector.poison",
    "art.defences.postprocessor",
    "art.defences.preprocessor",
    "art.defences.trainer",
    "art.defences.transformer",
    "art.estimators",
    "art.estimators.certification",
    "art.estimators.certification.randomized_smoothing",
    "art.estimators.classification",
    "art.estimators.encoding",
    "art.estimators.generation",
    "art.estimators.object_detection",
    "art.estimators.regression",
    "art.metrics",
    "art.wrappers",
    "tests",
    "tests.attacks",
    "tests.attacks.evasion",
    "tests.attacks.inference",
    "tests.classifiersFrameworks",
    "tests.defences",
    "tests.defences.detector",
    "tests.defences.detector.evasion",
    "tests.defences.detector.evasion.subsetscanning",
    "tests.defences.detector.poison",
    "tests.estimators",
    "tests.estimators.certification",
    "tests.estimators.classification",
    "tests.metrics",
    "tests.wrappers"
   ]
  }
 },
 "linux_64_requirements":{
  "build":{
   "__set__":true,
   "elements":[]
  },
  "host":{
   "__set__":true,
   "elements":[
    "pip",
    "python"
   ]
  },
  "run":{
   "__set__":true,
   "elements":[
    "cma",
    "ffmpeg-python",
    "matplotlib-base",
    "mypy",
    "pillow",
    "pydub",
    "python",
    "resampy",
    "scikit-learn",
    "scipy",
    "six",
    "statsmodels",
    "tqdm"
   ]
  },
  "test":{
   "__set__":true,
   "elements":[]
  }
 },
 "meta_yaml":{
  "about":{
   "dev_url":"https://github.com/Trusted-AI/adversarial-robustness-toolbox",
   "doc_url":"https://github.com/Trusted-AI/adversarial-robustness-toolbox/wiki/Documentation",
   "home":"https://github.com/Trusted-AI/adversarial-robustness-toolbox",
   "license":"MIT",
   "license_family":"MIT",
   "license_file":"LICENSE",
   "summary":"Toolbox for adversarial machine learning."
  },
  "build":{
   "noarch":"python",
   "number":"0",
   "script":"-m pip install . -vv"
  },
  "extra":{
   "recipe-maintainers":[
    "rluria14",
    "oblute",
    "ndmaxar"
   ]
  },
  "package":{
   "name":"adversarial-robustness-toolbox",
   "version":"1.3.3"
  },
  "requirements":{
   "host":[
    "pip",
    "python"
   ],
   "run":[
    "cma",
    "ffmpeg-python",
    "matplotlib-base",
    "mypy",
    "pillow",
    "pydub",
    "python",
    "resampy",
    "scikit-learn ==0.22.1",
    "scipy ==1.4.1",
    "six",
    "statsmodels",
    "tqdm"
   ]
  },
  "source":{
   "sha256":"204e9db634e39fb05ab72fbfbd4762ec71ac085b22264c2718e376be111b08d1",
   "url":"https://pypi.io/packages/source/a/adversarial-robustness-toolbox/adversarial-robustness-toolbox-1.3.3.tar.gz"
  },
  "test":{
   "imports":[
    "art",
    "art.attacks",
    "art.attacks.evasion",
    "art.attacks.evasion.adversarial_patch",
    "art.attacks.evasion.projected_gradient_descent",
    "art.attacks.extraction",
    "art.attacks.inference",
    "art.attacks.poisoning",
    "art.attacks.poisoning.perturbations",
    "art.classifiers",
    "art.classifiers.scikitlearn",
    "art.defences",
    "art.defences.detector",
    "art.defences.detector.evasion",
    "art.defences.detector.evasion.subsetscanning",
    "art.defences.detector.poison",
    "art.defences.postprocessor",
    "art.defences.preprocessor",
    "art.defences.trainer",
    "art.defences.transformer",
    "art.estimators",
    "art.estimators.certification",
    "art.estimators.certification.randomized_smoothing",
    "art.estimators.classification",
    "art.estimators.encoding",
    "art.estimators.generation",
    "art.estimators.object_detection",
    "art.estimators.regression",
    "art.metrics",
    "art.wrappers",
    "tests",
    "tests.attacks",
    "tests.attacks.evasion",
    "tests.attacks.inference",
    "tests.classifiersFrameworks",
    "tests.defences",
    "tests.defences.detector",
    "tests.defences.detector.evasion",
    "tests.defences.detector.evasion.subsetscanning",
    "tests.defences.detector.poison",
    "tests.estimators",
    "tests.estimators.certification",
    "tests.estimators.classification",
    "tests.metrics",
    "tests.wrappers"
   ]
  }
 },
 "name":"adversarial-robustness-toolbox",
 "new_version":"1.4.0",
 "new_version_attempts":{
  "1.3.2":1,
  "1.3.3":1
 },
 "new_version_errors":{},
 "osx_64_meta_yaml":{
  "about":{
   "dev_url":"https://github.com/Trusted-AI/adversarial-robustness-toolbox",
   "doc_url":"https://github.com/Trusted-AI/adversarial-robustness-toolbox/wiki/Documentation",
   "home":"https://github.com/Trusted-AI/adversarial-robustness-toolbox",
   "license":"MIT",
   "license_family":"MIT",
   "license_file":"LICENSE",
   "summary":"Toolbox for adversarial machine learning."
  },
  "build":{
   "noarch":"python",
   "number":"0",
   "script":" -m pip install . -vv"
  },
  "extra":{
   "recipe-maintainers":[
    "rluria14",
    "oblute",
    "ndmaxar"
   ]
  },
  "package":{
   "name":"adversarial-robustness-toolbox",
   "version":"1.3.1"
  },
  "requirements":{
   "host":[
    "pip",
    "python"
   ],
   "run":[
    "cma",
    "ffmpeg-python",
    "matplotlib-base",
    "mypy",
    "pillow",
    "pydub",
    "python",
    "resampy",
    "scikit-learn ==0.22.1",
    "scipy ==1.4.1",
    "six",
    "statsmodels",
    "tqdm"
   ]
  },
  "source":{
   "sha256":"a5ed157b8e119abee41d2e3a4a7a72ee5878b7a43657918ac407558d5110b516",
   "url":"https://pypi.io/packages/source/a/adversarial-robustness-toolbox/adversarial-robustness-toolbox-1.3.1.tar.gz"
  },
  "test":{
   "imports":[
    "art",
    "art.attacks",
    "art.attacks.evasion",
    "art.attacks.evasion.adversarial_patch",
    "art.attacks.evasion.projected_gradient_descent",
    "art.attacks.extraction",
    "art.attacks.inference",
    "art.attacks.poisoning",
    "art.attacks.poisoning.perturbations",
    "art.classifiers",
    "art.classifiers.scikitlearn",
    "art.defences",
    "art.defences.detector",
    "art.defences.detector.evasion",
    "art.defences.detector.evasion.subsetscanning",
    "art.defences.detector.poison",
    "art.defences.postprocessor",
    "art.defences.preprocessor",
    "art.defences.trainer",
    "art.defences.transformer",
    "art.estimators",
    "art.estimators.certification",
    "art.estimators.certification.randomized_smoothing",
    "art.estimators.classification",
    "art.estimators.encoding",
    "art.estimators.generation",
    "art.estimators.object_detection",
    "art.estimators.regression",
    "art.metrics",
    "art.wrappers",
    "tests",
    "tests.attacks",
    "tests.attacks.evasion",
    "tests.attacks.inference",
    "tests.classifiersFrameworks",
    "tests.defences",
    "tests.defences.detector",
    "tests.defences.detector.evasion",
    "tests.defences.detector.evasion.subsetscanning",
    "tests.defences.detector.poison",
    "tests.estimators",
    "tests.estimators.certification",
    "tests.estimators.classification",
    "tests.metrics",
    "tests.wrappers"
   ]
  }
 },
 "osx_64_requirements":{
  "build":{
   "__set__":true,
   "elements":[]
  },
  "host":{
   "__set__":true,
   "elements":[
    "pip",
    "python"
   ]
  },
  "run":{
   "__set__":true,
   "elements":[
    "cma",
    "ffmpeg-python",
    "matplotlib-base",
    "mypy",
    "pillow",
    "pydub",
    "python",
    "resampy",
    "scikit-learn",
    "scipy",
    "six",
    "statsmodels",
    "tqdm"
   ]
  },
  "test":{
   "__set__":true,
   "elements":[]
  }
 },
 "pinning_version":"2020.08.21.04.46.41",
 "raw_meta_yaml":"{% set name = \"adversarial-robustness-toolbox\" %}\n{% set version = \"1.3.3\" %}\n\npackage:\n  name: {{ name|lower }}\n  version: {{ version }}\n\nsource:\n  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/{{ name }}-{{ version }}.tar.gz\n  sha256: 204e9db634e39fb05ab72fbfbd4762ec71ac085b22264c2718e376be111b08d1\n\nbuild:\n  number: 0\n  noarch: python\n  script: {{ PYTHON }} -m pip install . -vv\n\nrequirements:\n  host:\n    - pip\n    - python\n  run:\n    - cma\n    - ffmpeg-python\n    - matplotlib-base\n    - mypy\n    - pillow\n    - pydub\n    - python\n    - resampy\n    - scikit-learn ==0.22.1\n    - scipy ==1.4.1\n    - six\n    - statsmodels\n    - tqdm\n\ntest:\n  imports:\n    - art\n    - art.attacks\n    - art.attacks.evasion\n    - art.attacks.evasion.adversarial_patch\n    - art.attacks.evasion.projected_gradient_descent\n    - art.attacks.extraction\n    - art.attacks.inference\n    - art.attacks.poisoning\n    - art.attacks.poisoning.perturbations\n    - art.classifiers\n    - art.classifiers.scikitlearn\n    - art.defences\n    - art.defences.detector\n    - art.defences.detector.evasion\n    - art.defences.detector.evasion.subsetscanning\n    - art.defences.detector.poison\n    - art.defences.postprocessor\n    - art.defences.preprocessor\n    - art.defences.trainer\n    - art.defences.transformer\n    - art.estimators\n    - art.estimators.certification\n    - art.estimators.certification.randomized_smoothing\n    - art.estimators.classification\n    - art.estimators.encoding\n    - art.estimators.generation\n    - art.estimators.object_detection\n    - art.estimators.regression\n    - art.metrics\n    - art.wrappers\n    - tests\n    - tests.attacks\n    - tests.attacks.evasion\n    - tests.attacks.inference\n    - tests.classifiersFrameworks\n    - tests.defences\n    - tests.defences.detector\n    - tests.defences.detector.evasion\n    - tests.defences.detector.evasion.subsetscanning\n    - tests.defences.detector.poison\n    - tests.estimators\n    - tests.estimators.certification\n    - tests.estimators.classification\n    - tests.metrics\n    - tests.wrappers\n\nabout:\n  home: https://github.com/Trusted-AI/adversarial-robustness-toolbox\n  license: MIT\n  license_family: MIT\n  license_file: LICENSE\n  summary: Toolbox for adversarial machine learning.\n  doc_url: https://github.com/Trusted-AI/adversarial-robustness-toolbox/wiki/Documentation\n  dev_url: https://github.com/Trusted-AI/adversarial-robustness-toolbox\n\nextra:\n  recipe-maintainers:\n    - rluria14\n    - oblute\n    - ndmaxar\n",
 "req":{
  "__set__":true,
  "elements":[
   "cma",
   "ffmpeg-python",
   "matplotlib-base",
   "mypy",
   "pillow",
   "pip",
   "pydub",
   "python",
   "resampy",
   "scikit-learn",
   "scipy",
   "six",
   "statsmodels",
   "tqdm"
  ]
 },
 "requirements":{
  "build":{
   "__set__":true,
   "elements":[]
  },
  "host":{
   "__set__":true,
   "elements":[
    "pip",
    "python"
   ]
  },
  "run":{
   "__set__":true,
   "elements":[
    "cma",
    "ffmpeg-python",
    "matplotlib-base",
    "mypy",
    "pillow",
    "pydub",
    "python",
    "resampy",
    "scikit-learn",
    "scipy",
    "six",
    "statsmodels",
    "tqdm"
   ]
  },
  "test":{
   "__set__":true,
   "elements":[]
  }
 },
 "smithy_version":"No azure token. Create a token and\nput it in ~/.conda-smithy/azure.token\n3.7.10",
 "strong_exports":false,
 "total_requirements":{
  "build":{
   "__set__":true,
   "elements":[]
  },
  "host":{
   "__set__":true,
   "elements":[
    "pip",
    "python"
   ]
  },
  "run":{
   "__set__":true,
   "elements":[
    "cma",
    "ffmpeg-python",
    "matplotlib-base",
    "mypy",
    "pillow",
    "pydub",
    "python",
    "resampy",
    "scikit-learn ==0.22.1",
    "scipy ==1.4.1",
    "six",
    "statsmodels",
    "tqdm"
   ]
  },
  "test":{
   "__set__":true,
   "elements":[]
  }
 },
 "url":"https://pypi.io/packages/source/a/adversarial-robustness-toolbox/adversarial-robustness-toolbox-1.3.1.tar.gz",
 "version":"1.3.3",
 "win_64_meta_yaml":{
  "about":{
   "dev_url":"https://github.com/Trusted-AI/adversarial-robustness-toolbox",
   "doc_url":"https://github.com/Trusted-AI/adversarial-robustness-toolbox/wiki/Documentation",
   "home":"https://github.com/Trusted-AI/adversarial-robustness-toolbox",
   "license":"MIT",
   "license_family":"MIT",
   "license_file":"LICENSE",
   "summary":"Toolbox for adversarial machine learning."
  },
  "build":{
   "noarch":"python",
   "number":"0",
   "script":" -m pip install . -vv"
  },
  "extra":{
   "recipe-maintainers":[
    "rluria14",
    "oblute",
    "ndmaxar"
   ]
  },
  "package":{
   "name":"adversarial-robustness-toolbox",
   "version":"1.3.1"
  },
  "requirements":{
   "host":[
    "pip",
    "python"
   ],
   "run":[
    "cma",
    "ffmpeg-python",
    "matplotlib-base",
    "mypy",
    "pillow",
    "pydub",
    "python",
    "resampy",
    "scikit-learn ==0.22.1",
    "scipy ==1.4.1",
    "six",
    "statsmodels",
    "tqdm"
   ]
  },
  "source":{
   "sha256":"a5ed157b8e119abee41d2e3a4a7a72ee5878b7a43657918ac407558d5110b516",
   "url":"https://pypi.io/packages/source/a/adversarial-robustness-toolbox/adversarial-robustness-toolbox-1.3.1.tar.gz"
  },
  "test":{
   "imports":[
    "art",
    "art.attacks",
    "art.attacks.evasion",
    "art.attacks.evasion.adversarial_patch",
    "art.attacks.evasion.projected_gradient_descent",
    "art.attacks.extraction",
    "art.attacks.inference",
    "art.attacks.poisoning",
    "art.attacks.poisoning.perturbations",
    "art.classifiers",
    "art.classifiers.scikitlearn",
    "art.defences",
    "art.defences.detector",
    "art.defences.detector.evasion",
    "art.defences.detector.evasion.subsetscanning",
    "art.defences.detector.poison",
    "art.defences.postprocessor",
    "art.defences.preprocessor",
    "art.defences.trainer",
    "art.defences.transformer",
    "art.estimators",
    "art.estimators.certification",
    "art.estimators.certification.randomized_smoothing",
    "art.estimators.classification",
    "art.estimators.encoding",
    "art.estimators.generation",
    "art.estimators.object_detection",
    "art.estimators.regression",
    "art.metrics",
    "art.wrappers",
    "tests",
    "tests.attacks",
    "tests.attacks.evasion",
    "tests.attacks.inference",
    "tests.classifiersFrameworks",
    "tests.defences",
    "tests.defences.detector",
    "tests.defences.detector.evasion",
    "tests.defences.detector.evasion.subsetscanning",
    "tests.defences.detector.poison",
    "tests.estimators",
    "tests.estimators.certification",
    "tests.estimators.classification",
    "tests.metrics",
    "tests.wrappers"
   ]
  }
 },
 "win_64_requirements":{
  "build":{
   "__set__":true,
   "elements":[]
  },
  "host":{
   "__set__":true,
   "elements":[
    "pip",
    "python"
   ]
  },
  "run":{
   "__set__":true,
   "elements":[
    "cma",
    "ffmpeg-python",
    "matplotlib-base",
    "mypy",
    "pillow",
    "pydub",
    "python",
    "resampy",
    "scikit-learn",
    "scipy",
    "six",
    "statsmodels",
    "tqdm"
   ]
  },
  "test":{
   "__set__":true,
   "elements":[]
  }
 }
}