{
 "PRed": [
  {
   "PR": {
    "__lazy_json__": "pr_json/416899054.json"
   },
   "data": {
    "bot_rerun": 1603216859.5193415,
    "migrator_name": "MigrationYaml",
    "migrator_object_version": 1,
    "migrator_version": 0,
    "name": "pypy"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_object_version",
    "migrator_version",
    "name"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/416901248.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "MigrationYaml",
    "migrator_object_version": 1,
    "migrator_version": 0,
    "name": "python38"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_object_version",
    "migrator_version",
    "name"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/499553742.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "MigrationYaml",
    "migrator_object_version": 1,
    "migrator_version": 0,
    "name": "cuda110"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_object_version",
    "migrator_version",
    "name"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/507242973.json"
   },
   "data": {
    "bot_rerun": 1607781209.9384518,
    "migrator_name": "MigrationYaml",
    "migrator_object_version": 1,
    "migrator_version": 0,
    "name": "pypy"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_object_version",
    "migrator_version",
    "name"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/519102519.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "MigrationYaml",
    "migrator_object_version": 2,
    "migrator_version": 0,
    "name": "python39"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_object_version",
    "migrator_version",
    "name"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/525916989.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "MigrationYaml",
    "migrator_object_version": 1,
    "migrator_version": 0,
    "name": "windows_cuda"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_object_version",
    "migrator_version",
    "name"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/553135505.json"
   },
   "data": {
    "bot_rerun": 1613557352.6851385,
    "migrator_name": "MigrationYaml",
    "migrator_object_version": 1,
    "migrator_version": 0,
    "name": "pypy"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_object_version",
    "migrator_version",
    "name"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/574670722.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "MigrationYaml",
    "migrator_object_version": 1,
    "migrator_version": 0,
    "name": "cuda111_112"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_object_version",
    "migrator_version",
    "name"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/574847014.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "MigrationYaml",
    "migrator_object_version": 1,
    "migrator_version": 0,
    "name": "pypy"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_object_version",
    "migrator_version",
    "name"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/599108635.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "OSXArm",
    "migrator_version": 1,
    "name": "arm osx addition"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "name"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/650833806.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "MigrationYaml",
    "migrator_object_version": 1,
    "migrator_version": 0,
    "name": "pypy37"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_object_version",
    "migrator_version",
    "name"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/776198411.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "MigrationYaml",
    "migrator_object_version": 1,
    "migrator_version": 0,
    "name": "python310"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_object_version",
    "migrator_version",
    "name"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/888156692.json"
   },
   "data": {
    "bot_rerun": 1648862011.0530767,
    "migrator_name": "MigrationYaml",
    "migrator_object_version": 1,
    "migrator_version": 0,
    "name": "cuda_112_ppc64le_aarch64"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_object_version",
    "migrator_version",
    "name"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/898035593.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "ArchRebuild",
    "migrator_version": 1,
    "name": "aarch64 and ppc64le addition"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_version",
    "name"
   ]
  },
  {
   "PR": {
    "__lazy_json__": "pr_json/1008298738.json"
   },
   "data": {
    "bot_rerun": false,
    "migrator_name": "MigrationYaml",
    "migrator_object_version": 1,
    "migrator_version": 0,
    "name": "pypy38"
   },
   "keys": [
    "bot_rerun",
    "migrator_name",
    "migrator_object_version",
    "migrator_version",
    "name"
   ]
  }
 ],
 "archived": false,
 "bad": "make_graph: render error No module named 'toml'\nTraceback (most recent call last):\n  File \"/home/runner/work/autotick-bot/autotick-bot/cf-scripts/conda_forge_tick/feedstock_parser.py\", line 241, in populate_feedstock_attributes\n    parse_meta_yaml(\n  File \"/home/runner/work/autotick-bot/autotick-bot/cf-scripts/conda_forge_tick/utils.py\", line 167, in parse_meta_yaml\n    return _parse_meta_yaml_impl(\n  File \"/home/runner/work/autotick-bot/autotick-bot/cf-scripts/conda_forge_tick/utils.py\", line 239, in _parse_meta_yaml_impl\n    m = MetaData(tmpdir, config=config, variant=var)\n  File \"/usr/share/miniconda3/envs/run_env/lib/python3.9/site-packages/conda_build/metadata.py\", line 932, in __init__\n    self.parse_again(permit_undefined_jinja=True, allow_no_other_outputs=True)\n  File \"/usr/share/miniconda3/envs/run_env/lib/python3.9/site-packages/conda_build/metadata.py\", line 1007, in parse_again\n    self.meta = parse(self._get_contents(permit_undefined_jinja,\n  File \"/usr/share/miniconda3/envs/run_env/lib/python3.9/site-packages/conda_build/metadata.py\", line 1546, in _get_contents\n    from conda_build.jinja_context import context_processor, UndefinedNeverFail, FilteredLoader\n  File \"/usr/share/miniconda3/envs/run_env/lib/python3.9/site-packages/conda_build/jinja_context.py\", line 13, in <module>\n    import toml\nModuleNotFoundError: No module named 'toml'\n",
 "branch": "main",
 "conda-forge.yml": {},
 "feedstock_name": "faiss-split",
 "hash_type": "sha256",
 "name": "faiss-split",
 "new_version": "1.7.2",
 "new_version_attempts": {
  "1.6.4": 118,
  "1.6.5": 262,
  "1.7.0": 13,
  "1.7.1": 106,
  "1.7.2": 142
 },
 "new_version_errors": {
  "1.6.4": "We found a problem parsing the recipe: \n\nexpected a single document in the stream\n  in \"<unicode string>\", line 9, column 5:\n        'AutoTune.h', 'clone_index.h', ' ... \n        ^ (line: 9)\nbut found another document\n  in \"<unicode string>\", line 9, column 17:\n        'AutoTune.h', 'clone_index.h', 'Clustering.h ... \n                    ^ (line: 9)",
  "1.6.5": "We found a problem parsing the recipe: \n\nexpected a single document in the stream\n  in \"<unicode string>\", line 10, column 5:\n        'AutoTune.h', 'Clustering.h', 'D ... \n        ^ (line: 10)\nbut found another document\n  in \"<unicode string>\", line 10, column 17:\n        'AutoTune.h', 'Clustering.h', 'DirectMap.h', ... \n                    ^ (line: 10)",
  "1.7.0": "We found a problem parsing the recipe: \n\nexpected a single document in the stream\n  in \"<unicode string>\", line 13, column 5:\n        'AutoTune.h', 'Clustering.h', 'D ... \n        ^ (line: 13)\nbut found another document\n  in \"<unicode string>\", line 13, column 17:\n        'AutoTune.h', 'Clustering.h', 'DirectMap.h', ... \n                    ^ (line: 13)",
  "1.7.1": "We found a problem parsing the recipe for version '1.7.1': \n\nParserError('while parsing a block mapping',   in \"<unicode string>\", line 140, column 7:\n          string__###conda-selector###__cu ... \n          ^ (line: 140), \"expected <block end>, but found '<scalar>'\",   in \"<unicode string>\", line 141, column 109:\n     ...  cuda_compiler_version|replace(\".\", \"\") }}h<{ PKG_HASH }}_<{ num ... \n                                         ^ (line: 141))\n\ntraceback:\n  File \"/home/runner/work/autotick-bot/autotick-bot/cf-scripts/conda_forge_tick/migrators/version.py\", line 508, in migrate\n    cmeta = CondaMetaYAML(fp.read())\n  File \"/home/runner/work/autotick-bot/autotick-bot/cf-scripts/conda_forge_tick/recipe_parser/_parser.py\", line 465, in __init__\n    self.meta = self._parser.load(\"\".join(lines))\n  File \"/usr/share/miniconda3/envs/run_env/lib/python3.9/site-packages/ruamel/yaml/main.py\", line 434, in load\n    return constructor.get_single_data()\n  File \"/usr/share/miniconda3/envs/run_env/lib/python3.9/site-packages/ruamel/yaml/constructor.py\", line 120, in get_single_data\n    node = self.composer.get_single_node()\n  File \"/usr/share/miniconda3/envs/run_env/lib/python3.9/site-packages/ruamel/yaml/composer.py\", line 76, in get_single_node\n    document = self.compose_document()\n  File \"/usr/share/miniconda3/envs/run_env/lib/python3.9/site-packages/ruamel/yaml/composer.py\", line 99, in compose_document\n    node = self.compose_node(None, None)\n  File \"/usr/share/miniconda3/envs/run_env/lib/python3.9/site-packages/ruamel/yaml/composer.py\", line 143, in compose_node\n    node = self.compose_mapping_node(anchor)\n  File \"/usr/share/miniconda3/envs/run_env/lib/python3.9/site-packages/ruamel/yaml/composer.py\", line 223, in compose_mapping_node\n    item_value = self.compose_node(node, item_key)\n  File \"/usr/share/miniconda3/envs/run_env/lib/python3.9/site-packages/ruamel/yaml/composer.py\", line 141, in compose_node\n    node = self.compose_sequence_node(anchor)\n  File \"/usr/share/miniconda3/envs/run_env/lib/python3.9/site-packages/ruamel/yaml/composer.py\", line 185, in compose_sequence_node\n    node.value.append(self.compose_node(node, index))\n  File \"/usr/share/miniconda3/envs/run_env/lib/python3.9/site-packages/ruamel/yaml/composer.py\", line 143, in compose_node\n    node = self.compose_mapping_node(anchor)\n  File \"/usr/share/miniconda3/envs/run_env/lib/python3.9/site-packages/ruamel/yaml/composer.py\", line 223, in compose_mapping_node\n    item_value = self.compose_node(node, item_key)\n  File \"/usr/share/miniconda3/envs/run_env/lib/python3.9/site-packages/ruamel/yaml/composer.py\", line 143, in compose_node\n    node = self.compose_mapping_node(anchor)\n  File \"/usr/share/miniconda3/envs/run_env/lib/python3.9/site-packages/ruamel/yaml/composer.py\", line 216, in compose_mapping_node\n    while not self.parser.check_event(MappingEndEvent):\n  File \"/usr/share/miniconda3/envs/run_env/lib/python3.9/site-packages/ruamel/yaml/parser.py\", line 146, in check_event\n    self.current_event = self.state()\n  File \"/usr/share/miniconda3/envs/run_env/lib/python3.9/site-packages/ruamel/yaml/parser.py\", line 611, in parse_block_mapping_key\n    raise ParserError(\n",
  "1.7.2": "We found a problem parsing the recipe for version '1.7.2': \n\nParserError('while parsing a block mapping',   in \"<unicode string>\", line 126, column 7:\n          string__###conda-selector###__cu ... \n          ^ (line: 126), \"expected <block end>, but found '<scalar>'\",   in \"<unicode string>\", line 127, column 109:\n     ...  cuda_compiler_version|replace(\".\", \"\") }}h<{ PKG_HASH }}_<{ PKG ... \n                                         ^ (line: 127))\n\ntraceback:\n  File \"/home/runner/work/autotick-bot/autotick-bot/cf-scripts/conda_forge_tick/migrators/version.py\", line 516, in migrate\n    cmeta = CondaMetaYAML(fp.read())\n  File \"/home/runner/work/autotick-bot/autotick-bot/cf-scripts/conda_forge_tick/recipe_parser/_parser.py\", line 494, in __init__\n    self.meta = self._parser.load(\"\".join(lines))\n  File \"/usr/share/miniconda3/envs/run_env/lib/python3.9/site-packages/ruamel/yaml/main.py\", line 434, in load\n    return constructor.get_single_data()\n  File \"/usr/share/miniconda3/envs/run_env/lib/python3.9/site-packages/ruamel/yaml/constructor.py\", line 119, in get_single_data\n    node = self.composer.get_single_node()\n  File \"/usr/share/miniconda3/envs/run_env/lib/python3.9/site-packages/ruamel/yaml/composer.py\", line 76, in get_single_node\n    document = self.compose_document()\n  File \"/usr/share/miniconda3/envs/run_env/lib/python3.9/site-packages/ruamel/yaml/composer.py\", line 99, in compose_document\n    node = self.compose_node(None, None)\n  File \"/usr/share/miniconda3/envs/run_env/lib/python3.9/site-packages/ruamel/yaml/composer.py\", line 143, in compose_node\n    node = self.compose_mapping_node(anchor)\n  File \"/usr/share/miniconda3/envs/run_env/lib/python3.9/site-packages/ruamel/yaml/composer.py\", line 223, in compose_mapping_node\n    item_value = self.compose_node(node, item_key)\n  File \"/usr/share/miniconda3/envs/run_env/lib/python3.9/site-packages/ruamel/yaml/composer.py\", line 141, in compose_node\n    node = self.compose_sequence_node(anchor)\n  File \"/usr/share/miniconda3/envs/run_env/lib/python3.9/site-packages/ruamel/yaml/composer.py\", line 185, in compose_sequence_node\n    node.value.append(self.compose_node(node, index))\n  File \"/usr/share/miniconda3/envs/run_env/lib/python3.9/site-packages/ruamel/yaml/composer.py\", line 143, in compose_node\n    node = self.compose_mapping_node(anchor)\n  File \"/usr/share/miniconda3/envs/run_env/lib/python3.9/site-packages/ruamel/yaml/composer.py\", line 223, in compose_mapping_node\n    item_value = self.compose_node(node, item_key)\n  File \"/usr/share/miniconda3/envs/run_env/lib/python3.9/site-packages/ruamel/yaml/composer.py\", line 143, in compose_node\n    node = self.compose_mapping_node(anchor)\n  File \"/usr/share/miniconda3/envs/run_env/lib/python3.9/site-packages/ruamel/yaml/composer.py\", line 216, in compose_mapping_node\n    while not self.parser.check_event(MappingEndEvent):\n  File \"/usr/share/miniconda3/envs/run_env/lib/python3.9/site-packages/ruamel/yaml/parser.py\", line 146, in check_event\n    self.current_event = self.state()\n  File \"/usr/share/miniconda3/envs/run_env/lib/python3.9/site-packages/ruamel/yaml/parser.py\", line 612, in parse_block_mapping_key\n    raise ParserError(\n"
 },
 "outputs_names": {
  "__set__": true,
  "elements": [
   "faiss",
   "faiss-cpu",
   "faiss-gpu",
   "faiss-proc",
   "libfaiss",
   "libfaiss-avx2"
  ]
 },
 "pinning_version": "2022.07.25.22.44.11",
 "pre_pr_migrator_attempts": {
  "python39": 1
 },
 "pre_pr_migrator_status": {
  "python39": "not solvable: master: ['linux_64_cuda_compiler_version10.0: Encountered problems while solving.\\nProblem: package numpy-1.14.0-py27h3dfced4_0 requires python >=2.7,<2.8.0a0, but none of the providers can be installed\\n', 'linux_64_cuda_compiler_version10.1: Encountered problems while solving.\\nProblem: package numpy-1.14.0-py27h3dfced4_0 requires python >=2.7,<2.8.0a0, but none of the providers can be installed\\n', 'linux_64_cuda_compiler_version10.2: Encountered problems while solving.\\nProblem: package numpy-1.14.0-py27h3dfced4_0 requires python >=2.7,<2.8.0a0, but none of the providers can be installed\\n', 'linux_64_cuda_compiler_version11.0: Encountered problems while solving.\\nProblem: package numpy-1.14.0-py27h3dfced4_0 requires python >=2.7,<2.8.0a0, but none of the providers can be installed\\n', 'linux_64_cuda_compiler_version9.2: Encountered problems while solving.\\nProblem: package numpy-1.14.0-py27h3dfced4_0 requires python >=2.7,<2.8.0a0, but none of the providers can be installed\\n', 'linux_64_cuda_compiler_versionNone: Encountered problems while solving.\\nProblem: package numpy-1.14.0-py27h3dfced4_0 requires python >=2.7,<2.8.0a0, but none of the providers can be installed\\n', 'osx_64_: Encountered problems while solving.\\nProblem: package numpy-1.14.0-py27h8a80b8c_0 requires python >=2.7,<2.8.0a0, but none of the providers can be installed\\n']"
 },
 "raw_meta_yaml": "{% set version = \"1.7.2\" %}\n# see github.com/conda-forge/conda-forge.github.io/issues/1059 for naming discussion\n{% set faiss_proc_type = \"cuda\" if cuda_compiler_version != \"None\" else \"cpu\" %}\n\n# headers for upstream-folders 'faiss/*.h', 'faiss/{impl,invlists,utils}/*.h',\n# see https://github.com/facebookresearch/faiss/blob/v{{ version }}/faiss/CMakeLists.txt;\n# gpu adds headers in 'faiss/gpu/*.h', 'faiss/gpu/{impl,utils}/*.(cu)?h'.\n# generated by:\n# ls faiss/{.,impl,invlists,utils} | grep -E \"h$\"\n# ls faiss/gpu/{.,impl,impl/scan,utils,utils/blockselect,utils/warpselect} | grep -E \"h$\"\n{% set headers = [\n    'AutoTune.h', 'Clustering.h', 'IVFlib.h', 'Index.h', 'Index2Layer.h',\n    'IndexAdditiveQuantizer.h', 'IndexBinary.h', 'IndexBinaryFlat.h', 'IndexBinaryFromFloat.h',\n    'IndexBinaryHNSW.h', 'IndexBinaryHash.h', 'IndexBinaryIVF.h', 'IndexFlat.h',\n    'IndexFlatCodes.h', 'IndexHNSW.h', 'IndexIVF.h', 'IndexIVFAdditiveQuantizer.h',\n    'IndexIVFFlat.h', 'IndexIVFPQ.h', 'IndexIVFPQFastScan.h', 'IndexIVFPQR.h',\n    'IndexIVFSpectralHash.h', 'IndexLSH.h', 'IndexLattice.h', 'IndexNNDescent.h', 'IndexNSG.h',\n    'IndexPQ.h', 'IndexPQFastScan.h', 'IndexPreTransform.h', 'IndexRefine.h', 'IndexReplicas.h',\n    'IndexScalarQuantizer.h', 'IndexShards.h', 'MatrixStats.h', 'MetaIndexes.h', 'MetricType.h',\n    'VectorTransform.h', 'clone_index.h', 'index_factory.h', 'index_io.h',\n    'impl/AdditiveQuantizer.h', 'impl/AuxIndexStructures.h', 'impl/FaissAssert.h',\n    'impl/FaissException.h', 'impl/HNSW.h', 'impl/LocalSearchQuantizer.h', 'impl/NNDescent.h',\n    'impl/NSG.h', 'impl/PolysemousTraining.h', 'impl/ProductQuantizer-inl.h',\n    'impl/ProductQuantizer.h', 'impl/ResidualQuantizer.h', 'impl/ResultHandler.h',\n    'impl/ScalarQuantizer.h', 'impl/ThreadedIndex-inl.h', 'impl/ThreadedIndex.h', 'impl/io.h',\n    'impl/io_macros.h', 'impl/kmeans1d.h', 'impl/lattice_Zn.h', 'impl/platform_macros.h',\n    'impl/pq4_fast_scan.h', 'impl/simd_result_handlers.h',\n    'invlists/BlockInvertedLists.h', 'invlists/DirectMap.h', 'invlists/InvertedLists.h',\n    'invlists/InvertedListsIOHook.h',\n    'utils/AlignedTable.h', 'utils/Heap.h', 'utils/WorkerThread.h', 'utils/distances.h',\n    'utils/extra_distances-inl.h', 'utils/extra_distances.h', 'utils/hamming-inl.h',\n    'utils/hamming.h', 'utils/ordered_key_value.h', 'utils/partitioning.h', 'utils/quantize_lut.h',\n    'utils/random.h', 'utils/simdlib.h', 'utils/simdlib_avx2.h', 'utils/simdlib_emulated.h',\n    'utils/simdlib_neon.h', 'utils/utils.h',\n] + (not win) * [\n    'invlists/OnDiskInvertedLists.h'\n] + (cuda_compiler_version != \"None\") * [\n    'gpu/GpuAutoTune.h', 'gpu/GpuCloner.h', 'gpu/GpuClonerOptions.h', 'gpu/GpuDistance.h',\n    'gpu/GpuFaissAssert.h', 'gpu/GpuIcmEncoder.h', 'gpu/GpuIndex.h', 'gpu/GpuIndexBinaryFlat.h',\n    'gpu/GpuIndexFlat.h', 'gpu/GpuIndexIVF.h', 'gpu/GpuIndexIVFFlat.h', 'gpu/GpuIndexIVFPQ.h',\n    'gpu/GpuIndexIVFScalarQuantizer.h', 'gpu/GpuIndicesOptions.h', 'gpu/GpuResources.h',\n    'gpu/StandardGpuResources.h',\n    'gpu/impl/BinaryDistance.cuh', 'gpu/impl/BinaryFlatIndex.cuh', 'gpu/impl/BroadcastSum.cuh',\n    'gpu/impl/Distance.cuh', 'gpu/impl/DistanceUtils.cuh', 'gpu/impl/FlatIndex.cuh',\n    'gpu/impl/GeneralDistance.cuh', 'gpu/impl/GpuScalarQuantizer.cuh', 'gpu/impl/IVFAppend.cuh',\n    'gpu/impl/IVFBase.cuh', 'gpu/impl/IVFFlat.cuh', 'gpu/impl/IVFFlatScan.cuh',\n    'gpu/impl/IVFInterleaved.cuh', 'gpu/impl/IVFPQ.cuh', 'gpu/impl/IVFUtils.cuh',\n    'gpu/impl/IcmEncoder.cuh', 'gpu/impl/InterleavedCodes.h', 'gpu/impl/L2Norm.cuh',\n    'gpu/impl/L2Select.cuh', 'gpu/impl/PQCodeDistances-inl.cuh', 'gpu/impl/PQCodeDistances.cuh',\n    'gpu/impl/PQCodeLoad.cuh', 'gpu/impl/PQScanMultiPassNoPrecomputed-inl.cuh',\n    'gpu/impl/PQScanMultiPassNoPrecomputed.cuh', 'gpu/impl/PQScanMultiPassPrecomputed.cuh',\n    'gpu/impl/RemapIndices.h', 'gpu/impl/VectorResidual.cuh',\n    'gpu/impl/scan/IVFInterleavedImpl.cuh',\n    'gpu/utils/BlockSelectKernel.cuh', 'gpu/utils/Comparators.cuh',\n    'gpu/utils/ConversionOperators.cuh', 'gpu/utils/CopyUtils.cuh', 'gpu/utils/DeviceDefs.cuh',\n    'gpu/utils/DeviceTensor-inl.cuh', 'gpu/utils/DeviceTensor.cuh', 'gpu/utils/DeviceUtils.h',\n    'gpu/utils/DeviceVector.cuh', 'gpu/utils/Float16.cuh', 'gpu/utils/HostTensor-inl.cuh',\n    'gpu/utils/HostTensor.cuh', 'gpu/utils/Limits.cuh', 'gpu/utils/LoadStoreOperators.cuh',\n    'gpu/utils/MathOperators.cuh', 'gpu/utils/MatrixMult-inl.cuh', 'gpu/utils/MatrixMult.cuh',\n    'gpu/utils/MergeNetworkBlock.cuh', 'gpu/utils/MergeNetworkUtils.cuh',\n    'gpu/utils/MergeNetworkWarp.cuh', 'gpu/utils/NoTypeTensor.cuh', 'gpu/utils/Pair.cuh',\n    'gpu/utils/PtxUtils.cuh', 'gpu/utils/ReductionOperators.cuh', 'gpu/utils/Reductions.cuh',\n    'gpu/utils/Select.cuh', 'gpu/utils/StackDeviceMemory.h', 'gpu/utils/StaticUtils.h',\n    'gpu/utils/Tensor-inl.cuh', 'gpu/utils/Tensor.cuh', 'gpu/utils/ThrustAllocator.cuh',\n    'gpu/utils/Timer.h', 'gpu/utils/Transpose.cuh', 'gpu/utils/WarpPackedBits.cuh',\n    'gpu/utils/WarpSelectKernel.cuh', 'gpu/utils/WarpShuffles.cuh',\n    'gpu/utils/blockselect/BlockSelectImpl.cuh', 'gpu/utils/warpselect/WarpSelectImpl.cuh'\n] %}\n\npackage:\n  name: faiss-split\n  version: {{ version }}\n\nsource:\n  url: https://github.com/facebookresearch/faiss/archive/v{{ version }}.tar.gz\n  sha256: d49b4afd6a7a5b64f260a236ee9b2efb760edb08c33d5ea5610c2f078a5995ec\n  patches:\n    - patches/0001-use-c-14.patch  # [cuda_compiler_version == \"10.2\"]\n    - patches/0001-use-c-17.patch  # [cuda_compiler_version != \"10.2\"]\n    # adapt header target directory for faiss_avx2\n    - patches/0002-adapt-header-target-directory-to-outputname.patch\n    # patch for avoiding crash in GPU test suite on windows\n    - patches/0003-skip-test_stress-for-GPU-on-windows.patch\n    # enable building libfaiss_avx2 without libfaiss\n    - patches/0004-enable-building-libfaiss_avx2-without-libfaiss.patch\n    # increase tolerance for test that occasionally fails marginally\n    - patches/0005-increase-tolerance-for-marginally-failing-test.patch\n    # add /bigobj on windows to avoid: \"fatal error C1128: number of sections exceeded object file format limit\"\n    - patches/0006-add-bigobj-to-swigfaiss-compile-options-on-windows.patch\n\nbuild:\n  number: 0\n\nrequirements:\n  build:\n    - {{ compiler('cxx') }}\n\noutputs:\n  # A meta-package to select CPU or GPU build for faiss.\n  - name: faiss-proc\n    version: 1.0.0\n    build:\n      string: {{ faiss_proc_type }}\n    test:\n      commands:\n        - exit 0\n\n  # build two separate C++ libs, one for generic x64, and one for AVX2\n  {% for CF_FAISS_BUILD in [\"avx2\", \"generic\"] %}\n  # order libfaiss last in loop due to conda/conda-build#4090; libfaiss-avx2\n  # is only used for faiss and not important enough to work-around for this bug\n  {% if CF_FAISS_BUILD == \"generic\" %}\n  - name: libfaiss\n  {% else %}\n  - name: libfaiss-avx2\n  {% endif %}\n  {% set libext = \"_avx2\" if CF_FAISS_BUILD == \"avx2\" else \"\" %}\n    # only one main build script build-lib.{bat|sh}, with the only difference\n    # through CF_FAISS_BUILD={generic,avx2} that's set in the wrappers\n    script: build-lib-{{ CF_FAISS_BUILD }}.sh   # [not win]\n    script: build-lib-{{ CF_FAISS_BUILD }}.bat  # [win]\n    build:\n      string: \"h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}_{{ faiss_proc_type }}\"                                                  # [cuda_compiler_version == \"None\"]\n      string: \"cuda{{ cuda_compiler_version|replace(\".\", \"\") }}h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}_{{ faiss_proc_type }}\"  # [cuda_compiler_version != \"None\"]\n      run_exports:\n        # faiss follows SemVer, so restrict packages built with libfaiss to use\n        # at least the same version at runtime, but below the next major version.\n        # (matches default arguments/behaviour of `pin_compatible`: min_pin='x.x.x.x.x.x', max_pin='x')\n        - {{ pin_compatible('libfaiss%s' % libext) }}\n        # additionally, we need to ensure matching proc-type\n        - libfaiss{{ libext }} =*=*_{{ faiss_proc_type }}\n    requirements:\n      build:\n        - {{ compiler('cxx') }}\n        - {{ compiler('cuda') }}  # [cuda_compiler_version != \"None\"]\n        - cmake\n        - make                    # [linux]\n        - libgomp                 # [linux]\n        - llvm-openmp             # [osx]\n      host:\n        - libblas\n        - liblapack\n      run_constrained:\n        - faiss-cpu ==9999999999  # [cuda_compiler_version != \"None\"]\n        - faiss-gpu ==9999999999  # [cuda_compiler_version == \"None\"]\n        - faiss-proc =*={{ faiss_proc_type }}\n\n    test:\n      commands:\n        # shared\n        - test -f $PREFIX/lib/libfaiss{{ libext }}.so               # [linux]\n        - test -f $PREFIX/lib/libfaiss{{ libext }}.dylib            # [osx]\n        - if not exist %LIBRARY_BIN%\\faiss{{ libext }}.dll exit 1   # [win]\n        # On windows, faiss.lib is an \"import library\";\n        # Deleting it breaks the faiss-builds\n        - if not exist %LIBRARY_LIB%\\faiss{{ libext }}.lib exit 1   # [win]\n\n        # absence of static libraries\n        - test ! -f $PREFIX/lib/libfaiss{{ libext }}.a              # [not win]\n\n        # headers\n        {% for each_header in headers %}\n        - test -f $PREFIX/include/faiss{{ libext }}/{{ each_header }} || (echo \"{{ each_header }} not found\" && exit 1)  # [unix]\n        - if not exist %LIBRARY_INC%\\faiss{{ libext }}\\{{ \"\\\\\".join(each_header.split(\"/\")) }} exit 1                    # [win]\n        {% endfor %}\n  {% endfor %}\n\n  - name: faiss\n    script: build-pkg.sh          # [not win]\n    script: build-pkg.bat         # [win]\n    build:\n      string: \"py{{ CONDA_PY }}h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}_{{ faiss_proc_type }}\"                                                  # [cuda_compiler_version == \"None\"]\n      string: \"py{{ CONDA_PY }}cuda{{ cuda_compiler_version|replace(\".\", \"\") }}h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}_{{ faiss_proc_type }}\"  # [cuda_compiler_version != \"None\"]\n    requirements:\n      build:\n        - {{ compiler('cxx') }}\n        - {{ compiler('cuda') }}  # [cuda_compiler_version != \"None\"]\n        - swig\n        - cmake\n        - make                    # [linux]\n        - libgomp                 # [linux]\n        - llvm-openmp             # [osx]\n      host:\n        - python\n        - pip\n        - numpy\n        - libfaiss ={{ version }}=*_{{ faiss_proc_type }}\n        - libfaiss-avx2 ={{ version }}=*_{{ faiss_proc_type }}\n        - libblas\n        - liblapack\n      run:\n        - python\n        - libfaiss ={{ version }}=*_{{ faiss_proc_type }}\n        - libfaiss-avx2 ={{ version }}=*_{{ faiss_proc_type }}\n        - {{ pin_compatible('numpy') }}\n      run_constrained:\n        - faiss-cpu ==9999999999  # [cuda_compiler_version != \"None\"]\n        - faiss-gpu ==9999999999  # [cuda_compiler_version == \"None\"]\n        - faiss-proc =*={{ faiss_proc_type }}\n\n    test:\n      requires:\n        # trying to test all blas-variants runs into conda/conda-build#3947\n        # - libblas =*=*{{ blas_impl }}\n        # testing with MKL, as upstream considers this the most important\n        - libblas =*=*mkl\n        - scipy\n        - pytest\n      files:\n        - test-pkg.bat\n        - test-pkg.sh\n      source_files:\n        - tests/\n      imports:\n        - faiss\n      commands:\n        # the linux & windows CI agents support AVX2 (OSX doesn't yet), so by default,\n        # we expect faiss will load the library with AVX2-support, see\n        # https://github.com/facebookresearch/faiss/blob/v1.7.1/faiss/python/loader.py#L52-L66\n        - export HAS_AVX2=YES && ./test-pkg.sh  # [linux]\n        - export HAS_AVX2=NO  && ./test-pkg.sh  # [osx]\n        # skip test suite on win + cuda < 11.2 due to time outs (note: \"None\" >= \"11.2\")\n        {% if cuda_compiler_version|string >= \"11.2\" %}\n        - set \"HAS_AVX2=YES\"  && test-pkg.bat   # [win]\n        {% endif %}\n\n        # running the following test requires an actual GPU device, which is not available in CI\n        # - pytest faiss/gpu/test/\n\n  # for compatibility with (& ease of migration from) existing packages in the pytorch channel\n  - name: faiss-cpu\n    build:\n      skip: true  # [cuda_compiler_version != \"None\"]\n    requirements:\n      run:\n        - faiss ={{ version }}=*_cpu\n    test:\n      imports:\n        - faiss\n\n  - name: faiss-gpu\n    build:\n      skip: true  # [cuda_compiler_version == \"None\"]\n    requirements:\n      run:\n        - faiss ={{ version }}=*_cuda\n    test:\n      imports:\n        - faiss\n\nabout:\n  home: https://github.com/facebookresearch/faiss\n  license: MIT\n  license_family: MIT\n  license_file: LICENSE\n  summary: 'A library for efficient similarity search and clustering of dense vectors.'\n\n  description: |\n    Faiss is a library for efficient similarity search and clustering of dense vectors.\n    It contains algorithms that search in sets of vectors of any size, up to ones that\n    possibly do not fit in RAM. It also contains supporting code for evaluation and\n    parameter tuning. Faiss is written in C++ with complete wrappers for Python/numpy.\n    Some of the most useful algorithms are implemented on the GPU. It is developed by\n    [Facebook AI Research](https://research.fb.com/category/facebook-ai-research-fair/).\n\n    For best performance, the maintainers of the package\n    [recommend](https://github.com/conda-forge/staged-recipes/pull/11337#issuecomment-623718460)\n    using the MKL implementation of blas/lapack. You can ensure that this is installed\n    by adding \"libblas =*=*mkl\" to your dependencies.\n  doc_url: https://rawgit.com/facebookresearch/faiss/master/docs/html/annotated.html\n  dev_url: https://github.com/facebookresearch/faiss\n\nextra:\n  recipe-maintainers:\n    - h-vetinari\n",
 "smithy_version": "3.21.0",
 "strong_exports": false,
 "url": "https://github.com/facebookresearch/faiss/archive/v1.7.2.tar.gz",
 "version": "1.7.2"
}